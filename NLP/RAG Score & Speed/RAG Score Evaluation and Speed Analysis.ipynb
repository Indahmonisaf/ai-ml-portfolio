{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "HhtaNuqwBFEB",
      "metadata": {
        "id": "HhtaNuqwBFEB"
      },
      "source": [
        "# Summery\n",
        "\n",
        "\n",
        "*   doing evaluation with scoring\n",
        "*   speed measurement\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "wO1CG3Hw-MuL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 8518,
          "status": "ok",
          "timestamp": 1733555672114,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "wO1CG3Hw-MuL",
        "outputId": "96c0fe48-d696-4b70-dbe3-7a3ebaa46c17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/6.5 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/6.5 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip3 install --upgrade --user --quiet google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ZyQRChcY-Sev",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 13,
          "status": "ok",
          "timestamp": 1733555672115,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "ZyQRChcY-Sev",
        "outputId": "a925de7f-57ce-4b1d-b3f7-b8d7c07422aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: google.colab.auth.authenticate_user() is not supported in Colab Enterprise.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "vQ80Of7j-VoK",
      "metadata": {
        "executionInfo": {
          "elapsed": 11801,
          "status": "ok",
          "timestamp": 1733555683904,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "vQ80Of7j-VoK"
      },
      "outputs": [],
      "source": [
        "# Define project information\n",
        "PROJECT_ID = \"monobrain-development\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "# Initialize Vertex AI\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "OT0UiYM7As6A",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2869,
          "status": "ok",
          "timestamp": 1733555686758,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "OT0UiYM7As6A",
        "outputId": "8089bc7b-e537-4491-d55c-de24e26d18e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (2.8.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.27.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.19.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.32.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (4.25.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.25.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage) (1.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2024.8.30)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-cloud-storage\n",
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "# バケット名とファイル名を指定\n",
        "bucket_name = 'not_confidencial_data_for_share'\n",
        "file_name = 'output_with_categories_1107.jsonl'\n",
        "local_file_path = 'output_with_categories_1107.jsonl'  # ローカルの保存先\n",
        "\n",
        "# Storageクライアントを初期化\n",
        "client = storage.Client()\n",
        "\n",
        "# バケットとBlobオブジェクトを取得\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "blob = bucket.blob(file_name)\n",
        "\n",
        "# ファイルをローカルにダウンロード\n",
        "blob.download_to_filename(local_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "bmsfZadu9uL2",
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1733558480775,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "bmsfZadu9uL2"
      },
      "outputs": [],
      "source": [
        "# バケット名とファイル名を指定\n",
        "bucket_name = 'not_confidencial_data_for_share'\n",
        "file_name = 'qa_pairs_for_rag_1108.jsonl'\n",
        "local_file_path = 'qa_pairs_for_rag_1108.jsonl'  # ローカルの保存先\n",
        "\n",
        "# Storageクライアントを初期化\n",
        "client = storage.Client()\n",
        "\n",
        "# バケットとBlobオブジェクトを取得\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "blob = bucket.blob(file_name)\n",
        "\n",
        "# ファイルをローカルにダウンロード\n",
        "blob.download_to_filename(local_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "W0DiFrvQG4at",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 8,
          "status": "ok",
          "timestamp": 1733555746196,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "W0DiFrvQG4at",
        "outputId": "2a03a5d3-376b-4c6b-85e1-602e453b06da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Collecting scann\n",
            "  Downloading scann-1.3.4-cp310-cp310-manylinux_2_27_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Collecting jsonlines\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: tensorflow~=2.17.0 in /usr/local/lib/python3.10/dist-packages (from scann) (2.17.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (24.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (69.5.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (1.67.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->scann) (0.37.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.17.0->scann) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow~=2.17.0->scann) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow~=2.17.0->scann) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow~=2.17.0->scann) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow~=2.17.0->scann) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow~=2.17.0->scann) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow~=2.17.0->scann) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow~=2.17.0->scann) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow~=2.17.0->scann) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow~=2.17.0->scann) (0.1.2)\n",
            "Downloading scann-1.3.4-cp310-cp310-manylinux_2_27_x86_64.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Installing collected packages: jsonlines, scann\n",
            "Successfully installed jsonlines-4.0.0 scann-1.3.4\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers scann numpy jsonlines"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IzChPz2GAg8A",
      "metadata": {
        "id": "IzChPz2GAg8A"
      },
      "source": [
        "# **BERT and ScaNN (Model 1)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iKtZE08w-YfN",
      "metadata": {
        "id": "iKtZE08w-YfN"
      },
      "source": [
        "With Category Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "WUg1_uG2HA_A",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496,
          "referenced_widgets": [
            "1c243e84376b43babbcb5ec8710d42d8",
            "cdc57923b49e443482f131b99722bb68",
            "2dc33a665681435597f659722c83f4e3",
            "c83a2e629e534b7da2d0576cabfc3ee8",
            "ab5b467838064f9a96aaa32f2edc7235",
            "9f566dfa5a4a48b083f1be84cc7364f8",
            "fa11b0b1127e4ce58200b020650e3eb1",
            "ba30dcdf5a624d53a4ef94f1ae1bc0b6",
            "7b925de0b7a54a82aeaa43fcfa50e8d6",
            "a2fca40cbf114558a6b1c15249f4a858",
            "d08dda38289844b8948d4742b318f8e1",
            "fb3b591be05848bba440fb16d7bb50e3",
            "c9dde708e85346bc918fca4031266a9c",
            "28a021db1dc84e05873e1f9921a0e966",
            "120253fbb8b74064b6439cd94ea41989",
            "617277c4261a441688cede0a894fd7ab",
            "798f81ce3a11472883022253e3f9be7a",
            "6270b408459441c388b2407530304001",
            "6695d01b036b4dcd9241ebc02ba9bd84",
            "696f0d4a35614eb1ad664ae74e30d10f",
            "6788add6209a406ca035087379758e2d",
            "c1ca330f928d4d19b1dcf5c5f6dddc5f",
            "4b1392e171c34a63977642d5d6222cca",
            "c6125c7cfc6f42e68bcfe8d99a6ae101",
            "5b3b5e96c64c4c749103a51faca29564",
            "80805b86c35d4441b613e610fca89c5b",
            "12c1dbb214d344bdb2d842cd86cdce81",
            "e8e3da8099e14d05acd0f2a78016ee73",
            "07664bd5c89d458cbdbd2747a8dabcd7",
            "0b018fa7f7f04dc79a3a456a37176311",
            "607d1adae85b4dd888c951766634eeea",
            "216a55ad64264f65ba029eeda3985068",
            "f1ba8b231dbd42abb0f241f4b789619b",
            "ff0f18db921049379b19b4ef3a69bd7d",
            "3923eb34261b42f2b03e4a9abd7aa015",
            "8a208d54a58a46449aab6630d92597f8",
            "b1c68c0250c4491c856565e8f5ab15b7",
            "002dc4cf2ddc4aeb9c55b0230d45d915",
            "2ef92d6f203f4f08bca926834fb7e27e",
            "1cb67c3043544c6c9bbb6ef2aae00214",
            "15317080ee48492d8629b45bf9a3ee30",
            "c409599ca9b6407487d50bac91c6b7fc",
            "d0f5f954c17e44169d8b926bac1723e0",
            "8771acf0881b40c9af2cf12ea2c641b3",
            "d0de0cdd14d74895b7b17b99f055af7f",
            "e6a25b6e705244fa886fdb8d3a234d30",
            "5b82ee55921a45e6b4210d718a397c97",
            "7a499d3c3bd44acb9245494cdded23dd",
            "16aa51485e064bd29ce2a0d996ee98c9",
            "575e8d4fdbd74adf81d07c0b86deff91",
            "abb59fa78b2841cb9880223bd00e000c",
            "94bc253a7fd148c995a86594dc9531c8",
            "89a5b315c4ae44798386bbfd3f24f562",
            "5307effdec754b389546936c6be3cd39",
            "b6ea2765292c4833971bba5c89404b27",
            "ab207e4a26d5485ca43cdf063580e200",
            "33b9239c886e4824bfcf96859f7bd044",
            "33d3b78566844cc6ace84e75cbfbf9ee",
            "88da377b810b4661aee30b4ca5488bb1",
            "0f3dda391efa491d860f84cddd35cfeb",
            "4c29d39267c54841b0e30ec850757610",
            "6b783163746944c692098934d167da30",
            "9b00b797db444fd2ad3bdc0454ccbd28",
            "ab46868dd50248b1b48969152f7ef44a",
            "3d29d22bc7714a8ab4379c35b9a28529",
            "cc1fd2d378e14ef094ba6945a6b96d17",
            "9ff3e97e24954d50b6e29da0fb34fcaf",
            "48d0249965ea419e914b42d0d4bc2376",
            "0ed05f681e2e4dde8ca663596390a2b8",
            "220dd27d11884f25b7c8ce37898cc0be",
            "4650042347de4bc9976d8f120585ee6c",
            "dad8c21eae1d4086a88aa40d610f0879",
            "cec67a62a22c44f3b7ccc361b7310d78",
            "7a609b4446e7484ea2391d12946fcc30",
            "f9700eba9d1a4753865d1d68eadc5607",
            "60f37db684a6412c99f03274c2e3db86",
            "8406e3f03fb346e78154b41c85602fa1",
            "dbda161a4612460989a1fec0a3c3e394",
            "e693eede878d4b0bb7ce407f8384c749",
            "5b75d641d63d41bc9d20228b03ea75c6",
            "1dda8dec1b354f52aaa1ac10d560a5e4",
            "18f858ecc5f743009752056bb027a002",
            "b669e601fa63473d906945fcc23a2672",
            "a979a729821d45bea4974f33dc5322b5",
            "00a8272da1c44389a7eed8979362c95a",
            "03573a319632410b83475cc9e330a9e0",
            "f5520b59454f4b11b5205873510ec8d2",
            "03c6cfac9654416fbcb060590ed65cc7",
            "4c3179e5f53a4f4dbe74ecf660dbfae0",
            "cd6043582e514292973f167e6ca59139",
            "1499209891d244e7ba43f50274cbeb43",
            "0b95c5bf7b4f46bb99b3a8826b1245d3",
            "785ad1afa516443889de176fc1128ad6",
            "8a987f3b81fc402199bcf7b4b17e46b5",
            "ffb6c8606c594a1e920b1c58b0e1dca3",
            "724364781c8a481085d156794f472ca7",
            "922568c834714d4daf851d465fdcf02a",
            "2298864a9e004152bc86d8f84018da8e",
            "46a7fc8c0daa46cab5f9a2ea93ef6dec",
            "ecce9b2400f84a059b4e25635c3d52d3",
            "f8897212a7eb4f478dcf00d1efd4056a",
            "5ad27bb6f44548a2b35a414a7d4632dd",
            "f682eb9eae9543ff813b6fc85e77763d",
            "4fe0ba1a050143b9af05234ee9f865dc",
            "eb0b723ee2fa42c1a2840d0cf78b4024",
            "309423df49364c8d8bb4625bee821491",
            "9dc761901bcc4bff89c5102e778f079c",
            "eaba3f2a336046b7b965b2a97754b5bb",
            "b4a0b997f1bb461b844f1901424879d4",
            "a20e2d7d190e42309342441e50aa3a60",
            "3083d0d2f93a4fb88d1d7923e6250182",
            "96a3a731ce874712bcb3f2e4e9215bf8",
            "cca6ede410f84dd1a191f2dbeef9ed89",
            "5889675bd9ca45988cb3ae2ba25eb95a",
            "102a48c4ef5649dfb8097449f6cb3780",
            "5bb6eccbf2c742dcb2dbfb8007764e50",
            "a19eadb9999a4f07b8101ec295e403c2",
            "fc9b0fb79a2c4f97adf046e8c5118202",
            "f9c42b8703e443e3b3009b86bfef09ff",
            "ee543581472549bdb07402045e331187",
            "c9b964bdec394594a92a2181afb3d54f"
          ]
        },
        "executionInfo": {
          "elapsed": 178232,
          "status": "ok",
          "timestamp": 1733555927122,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "WUg1_uG2HA_A",
        "outputId": "ef2bad42-cafd-4a82-c5de-c6016c37e980"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c243e84376b43babbcb5ec8710d42d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb3b591be05848bba440fb16d7bb50e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b1392e171c34a63977642d5d6222cca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff0f18db921049379b19b4ef3a69bd7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0de0cdd14d74895b7b17b99f055af7f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab207e4a26d5485ca43cdf063580e200",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ff3e97e24954d50b6e29da0fb34fcaf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbda161a4612460989a1fec0a3c3e394",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c3179e5f53a4f4dbe74ecf660dbfae0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ecce9b2400f84a059b4e25635c3d52d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3083d0d2f93a4fb88d1d7923e6250182",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average score of this model is: 2.01\n",
            "\n",
            "The average time taken for RAG retrieval is: 0.0399 seconds\n"
          ]
        }
      ],
      "source": [
        "import jsonlines\n",
        "import numpy as np\n",
        "import random\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import scann\n",
        "import base64\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part, SafetySetting\n",
        "import time\n",
        "import json\n",
        "\n",
        "# Step 1: Load your dataset (output_categories.jsonl)\n",
        "with jsonlines.open('output_with_categories_1107.jsonl') as reader:\n",
        "    qa_pairs = [obj for obj in reader]\n",
        "\n",
        "# Step 2: Prepare the question-answer-category pairs as documents\n",
        "documents = [f\"Question: {pair['question']} Answer: {pair['answer']} Category: {pair['category']}\" for pair in qa_pairs]\n",
        "questions = [pair['question'] for pair in qa_pairs]\n",
        "answers = [pair['answer'] for pair in qa_pairs]\n",
        "categories = [pair['category'] for pair in qa_pairs]\n",
        "\n",
        "# Initialize model for SBERT\n",
        "model_sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Step 3: Encode the documents using SBERT\n",
        "question_embeddings = model_sbert.encode(documents, convert_to_numpy=True)\n",
        "\n",
        "# Step 4: Create ScaNN index for SBERT embeddings\n",
        "scann_index = scann.scann_ops.builder(question_embeddings, 10, \"dot_product\").tree(num_leaves=min(1000, len(questions)), num_leaves_to_search=50).score_ah(2, anisotropic_quantization_threshold=0.2).reorder(50).build()\n",
        "\n",
        "# Select 20 random questions for evaluation\n",
        "evaluation_indices = random.sample(range(len(questions)), 20)\n",
        "evaluation_questions = [questions[i] for i in evaluation_indices]\n",
        "\n",
        "# Step 5: Function to retrieve the 5 most similar questions based on query\n",
        "def find_similar_questions(query, top_k=5):\n",
        "    # Encode the query using SBERT\n",
        "    query_embedding = model_sbert.encode([f\"Question: {query}\"], convert_to_numpy=True)\n",
        "\n",
        "    # Search the ScaNN index\n",
        "    neighbors, distances = scann_index.search_batched(query_embedding)\n",
        "\n",
        "    # Return the most similar questions\n",
        "    closest_questions = [questions[i] for i in neighbors[0][:top_k]]\n",
        "    return closest_questions\n",
        "\n",
        "# Function to assess similarity between two queries using Vertex AI\n",
        "def similarity_assessment_model(query1, query2):\n",
        "    generation_config = {\n",
        "        \"max_output_tokens\": 8192,\n",
        "        \"temperature\": 1,\n",
        "        \"top_p\": 0.95,\n",
        "        \"response_mime_type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    vertexai.init(project=\"monobrain-development\", location=\"us-central1\")\n",
        "    model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
        "    responses = model.generate_content([\n",
        "        \"\"\"あなたは日本語の小説の編集者をしており、文章のスペシャリストです。\n",
        "        以下の文章を比べて類似度を数値5段階で出力してください。\\n\\n\n",
        "        文章の大まかな内容と細かい内容、文章表現全てが類似している場合は : 5\n",
        "        文章の大まかな内容は類似しているが、細かい内容が違っており、文章表現が類似しているものは : 4\n",
        "        文章の大まかな内容は類似しているが、細かい内容が違っており、文章表現が異なっているものは : 3\n",
        "        文章の大まかな内容は異なっているが、文内で使われている単語が似通っている場合 : 2\n",
        "        文書の内容は間違っているが、文章表現や文章の雰囲気が似ている場合 : 1\n",
        "        文章の内容が違っており、文章表現や単語も異なっている場合 : 0\n",
        "\n",
        "        jsonの形式は{\"similarity\":3,\"comment\":\"ここになぜそのような評価にしたかのコメント\"}\n",
        "\n",
        "        文章1 :\"\"\" + query1 + \"\\n 文章2 : \" + query2],\n",
        "        generation_config=generation_config,\n",
        "        stream=False,\n",
        "    )\n",
        "\n",
        "    score = float(json.loads(responses.candidates[0].content.parts[0].text)['similarity'])\n",
        "    return score\n",
        "\n",
        "# Evaluate the system with 20 selected questions using Word2Vec\n",
        "similarity_scores = []\n",
        "#time\n",
        "total_time = 0.0\n",
        "\n",
        "# Iterate over each record to find the top 5 similar questions and calculate similarity scores + time for rag\n",
        "for record in evaluation_questions:\n",
        "    start_time = time.time()\n",
        "    similar5data = find_similar_questions(record)\n",
        "    end_time = time.time()\n",
        "    time_for_using_rag = end_time - start_time\n",
        "    total_time += time_for_using_rag\n",
        "    for one_data in similar5data:\n",
        "        score = similarity_assessment_model(record, one_data)\n",
        "        similarity_scores.append(score)\n",
        "\n",
        "# Calculate the average similarity score\n",
        "if similarity_scores:\n",
        "    average_similarity = np.mean(similarity_scores)\n",
        "    print(f\"\\nThe average score of this model is: {average_similarity}\")\n",
        "else:\n",
        "    print(\"\\nNo similarity scores were calculated.\")\n",
        "\n",
        "# Calculate the average time taken for RAG retrieval\n",
        "average_time = total_time / len(evaluation_questions)\n",
        "print(f\"\\nThe average time taken for RAG retrieval is: {average_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_Z0Wl3rz-l_d",
      "metadata": {
        "id": "_Z0Wl3rz-l_d"
      },
      "source": [
        "Without Category Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "EY1Ww9_e-0lT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 104772,
          "status": "ok",
          "timestamp": 1733556031891,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "EY1Ww9_e-0lT",
        "outputId": "132fac3a-786c-437e-fb93-a394436d0a14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average score of this model is: 2.27\n",
            "\n",
            "The average time taken for RAG retrieval is: 0.0274 seconds\n"
          ]
        }
      ],
      "source": [
        "import jsonlines\n",
        "import numpy as np\n",
        "import random\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import scann\n",
        "import base64\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part, SafetySetting\n",
        "import time\n",
        "import json\n",
        "\n",
        "# Step 1: Load your dataset (output_categories.jsonl)\n",
        "with jsonlines.open('qa_pairs_for_rag_1108.jsonl') as reader:\n",
        "    qa_pairs = [obj for obj in reader]\n",
        "\n",
        "# Step 2: Prepare the question-answer-category pairs as documents\n",
        "documents = [f\"Question: {pair['question']} Answer: {pair['answer']}\" for pair in qa_pairs]\n",
        "questions = [pair['question'] for pair in qa_pairs]\n",
        "answers = [pair['answer'] for pair in qa_pairs]\n",
        "\n",
        "# Initialize model for SBERT\n",
        "model_sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Step 3: Encode the documents using SBERT\n",
        "question_embeddings = model_sbert.encode(documents, convert_to_numpy=True)\n",
        "\n",
        "# Step 4: Create ScaNN index for SBERT embeddings\n",
        "scann_index = scann.scann_ops.builder(question_embeddings, 10, \"dot_product\").tree(num_leaves=min(1000, len(questions)), num_leaves_to_search=50).score_ah(2, anisotropic_quantization_threshold=0.2).reorder(50).build()\n",
        "\n",
        "# Select 20 random questions for evaluation\n",
        "evaluation_indices = random.sample(range(len(questions)), 20)\n",
        "evaluation_questions = [questions[i] for i in evaluation_indices]\n",
        "\n",
        "# Step 5: Function to retrieve the 5 most similar questions based on query\n",
        "def find_similar_questions(query, top_k=5):\n",
        "    # Encode the query using SBERT\n",
        "    query_embedding = model_sbert.encode([f\"Question: {query}\"], convert_to_numpy=True)\n",
        "\n",
        "    # Search the ScaNN index\n",
        "    neighbors, distances = scann_index.search_batched(query_embedding)\n",
        "\n",
        "    # Return the most similar questions\n",
        "    closest_questions = [questions[i] for i in neighbors[0][:top_k]]\n",
        "    return closest_questions\n",
        "\n",
        "# Function to assess similarity between two queries using Vertex AI\n",
        "def similarity_assessment_model(query1, query2):\n",
        "    generation_config = {\n",
        "        \"max_output_tokens\": 8192,\n",
        "        \"temperature\": 1,\n",
        "        \"top_p\": 0.95,\n",
        "        \"response_mime_type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    vertexai.init(project=\"monobrain-development\", location=\"us-central1\")\n",
        "    model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
        "    responses = model.generate_content([\n",
        "        \"\"\"あなたは日本語の小説の編集者をしており、文章のスペシャリストです。\n",
        "        以下の文章を比べて類似度を数値5段階で出力してください。\\n\\n\n",
        "        文章の大まかな内容と細かい内容、文章表現全てが類似している場合は : 5\n",
        "        文章の大まかな内容は類似しているが、細かい内容が違っており、文章表現が類似しているものは : 4\n",
        "        文章の大まかな内容は類似しているが、細かい内容が違っており、文章表現が異なっているものは : 3\n",
        "        文章の大まかな内容は異なっているが、文内で使われている単語が似通っている場合 : 2\n",
        "        文書の内容は間違っているが、文章表現や文章の雰囲気が似ている場合 : 1\n",
        "        文章の内容が違っており、文章表現や単語も異なっている場合 : 0\n",
        "\n",
        "        jsonの形式は{\"similarity\":3,\"comment\":\"ここになぜそのような評価にしたかのコメント\"}\n",
        "\n",
        "        文章1 :\"\"\" + query1 + \"\\n 文章2 : \" + query2],\n",
        "        generation_config=generation_config,\n",
        "        stream=False,\n",
        "    )\n",
        "\n",
        "    score = float(json.loads(responses.candidates[0].content.parts[0].text)['similarity'])\n",
        "    return score\n",
        "\n",
        "# Evaluate the system with 20 selected questions using Word2Vec\n",
        "similarity_scores = []\n",
        "#time\n",
        "total_time = 0.0\n",
        "\n",
        "# Iterate over each record to find the top 5 similar questions and calculate similarity scores + time for rag\n",
        "for record in evaluation_questions:\n",
        "    start_time = time.time()\n",
        "    similar5data = find_similar_questions(record)\n",
        "    end_time = time.time()\n",
        "    time_for_using_rag = end_time - start_time\n",
        "    total_time += time_for_using_rag\n",
        "    for one_data in similar5data:\n",
        "        score = similarity_assessment_model(record, one_data)\n",
        "        similarity_scores.append(score)\n",
        "\n",
        "# Calculate the average similarity score\n",
        "if similarity_scores:\n",
        "    average_similarity = np.mean(similarity_scores)\n",
        "    print(f\"\\nThe average score of this model is: {average_similarity}\")\n",
        "else:\n",
        "    print(\"\\nNo similarity scores were calculated.\")\n",
        "\n",
        "# Calculate the average time taken for RAG retrieval\n",
        "average_time = total_time / len(evaluation_questions)\n",
        "print(f\"\\nThe average time taken for RAG retrieval is: {average_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q7tlprEzAsUY",
      "metadata": {
        "id": "Q7tlprEzAsUY"
      },
      "source": [
        "# **T5 & FAISS (Model 2)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "Bbr4uTUjIrKg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 10295,
          "status": "ok",
          "timestamp": 1733556107115,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "Bbr4uTUjIrKg",
        "outputId": "2df868f9-6232-4eaa-86d0-aeed4cf7893a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pUXvsYed-cMr",
      "metadata": {
        "id": "pUXvsYed-cMr"
      },
      "source": [
        "With Category Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "-XjRWv-0IvpM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248,
          "referenced_widgets": [
            "4082b8616fd242df88989c7237f65f96",
            "cc77df4437cd470f915b5fbdd2b3dcd4",
            "df0afdaa89524f49af9fdad33aa5289e",
            "5308b0fa981b49a798b3752131210016",
            "b723f3980860446abcbf721d2232fd68",
            "5238e20147e549d5b6c9a1e47606814b",
            "f61268963eb54a29a7d1766cadf865ce",
            "4d29e109659e475baeba9f71b225739a",
            "44e0228078d14dc3baec64746a85510a",
            "0e51072b44464262ad13eba8a25738e5",
            "1d589bf0026d4c3e9ef445553b513327",
            "0dd38f409f814a149bb38d10849fed55",
            "67c25d7615fe4d74a22e4c369c438ac6",
            "f961f24cc0744374a78ace131f8c5c16",
            "52a898a9287a4ee3be0f5fda421958e7",
            "6ea305f256f24963b76b1eb6a124ed55",
            "c95085874b3b4607b36a60da0cc47715",
            "fbdd4039ee774b108957799c38b40c48",
            "a2fb48e0d7e34a70a0b43fc5073c4064",
            "3e8b865e64544f3894f2032d574c1c95",
            "6ac0d4cd961c4639ace9fd92e2f3a57a",
            "2b21a9b5d580469a833ab045b5efc245",
            "3233b695ede4424ba7c6db23c3282607",
            "384fc7bc662d468b9a1fd6e2fc2c8ac7",
            "9b26f7895c484cc786ed08d087f43787",
            "d0f56b88827f4e3389ba1f4e6a89fb65",
            "1956d17da5e24a1eaa379b7978543f48",
            "7d93e7c6df72435cab97e2865561cc60",
            "f6ef2e22a45746c9afbfd04bf08a33a6",
            "db7876687e124a58af008b6ae7966ad7",
            "181fe8f0c37c44f6aa618c4dd2bfcd6e",
            "8b5edf6969874ffcaa49f8fec7697323",
            "8b79d2b2de7d48adb1716f4cfc7a03d0",
            "9f7eb1890e414ff6bd2c2a0eb44e2af8",
            "d9a85d4488bc4a63ba45b72a4fe7b9a1",
            "a64d32ede2b049ddbf66a999d675a228",
            "46a05b41155241248886a64f3c698741",
            "bec6588293da45f6bf75f33c9c3336d7",
            "9f05188122a74c7682837c5fefca0c9d",
            "914017c957014852aa038061a43348f9",
            "e6a6eecfa001448c8a2eff2fb2cf10f7",
            "f861ac6d42d5440ebb03aee214381516",
            "3f6fdd4977ef4eff9c26fd82f496381c",
            "5300ae0599df454bb0dcd47604d9cb0b",
            "5ab990ae9aa94981b2275c9f2892a90a",
            "bd24d61d7cd9421ebeb3caed723ae5a5",
            "c5a5d5cce99e4475a80503c0755b6183",
            "910efb71917d454b9cbdf3f7d604dd5f",
            "e81a07a040934423bf07ecb64793b7c1",
            "1b668d832566440c820433c83c1c594d",
            "c681df78da8f4d649af47aebc0c524b9",
            "4f64e0c3d75a45059105870eb7845549",
            "9a2c3b54d00d4d148cd49ef165e5e8b6",
            "06f8f16ea66547468d2c974681ab2527",
            "4fe36affe15243018005efe185195471"
          ]
        },
        "executionInfo": {
          "elapsed": 229248,
          "status": "ok",
          "timestamp": 1733556336356,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "-XjRWv-0IvpM",
        "outputId": "7ac6a012-1add-43b9-96a4-eea5382332c5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4082b8616fd242df88989c7237f65f96",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0dd38f409f814a149bb38d10849fed55",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3233b695ede4424ba7c6db23c3282607",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f7eb1890e414ff6bd2c2a0eb44e2af8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ab990ae9aa94981b2275c9f2892a90a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average score of this model is: 1.24\n",
            "\n",
            "The average time taken for RAG retrieval is: 0.0716 seconds\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import faiss\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import base64\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part, SafetySetting\n",
        "\n",
        "# Step 1: Load your dataset (output_categories.jsonl)\n",
        "with jsonlines.open('output_with_categories_1107.jsonl') as reader:\n",
        "    qa_pairs = [obj for obj in reader]\n",
        "\n",
        "# Step 2: Prepare the question-answer-category pairs as documents\n",
        "documents = [f\"Question: {pair['question']} Answer: {pair['answer']} Category: {pair['category']}\" for pair in qa_pairs]\n",
        "questions = [pair['question'] for pair in qa_pairs]\n",
        "answers = [pair['answer'] for pair in qa_pairs]\n",
        "categories = [pair['category'] for pair in qa_pairs]\n",
        "\n",
        "# Initialize tokenizer and model for T5\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
        "\n",
        "# Step 3: Function to generate embeddings using T5\n",
        "def generate_embedding(text):\n",
        "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        output = model.encoder(input_ids)\n",
        "    embeddings = output.last_hidden_state.mean(dim=1).squeeze().numpy()  # Average pooling\n",
        "    return embeddings\n",
        "\n",
        "# Step 4: Generate embeddings for each question\n",
        "question_embeddings = [generate_embedding(doc) for doc in documents]\n",
        "\n",
        "# Step 5: Initialize Faiss index\n",
        "d = question_embeddings[0].shape[0]  # Dimensionality of embeddings\n",
        "faiss_index = faiss.IndexFlatL2(d)\n",
        "faiss_index.add(np.array(question_embeddings))\n",
        "\n",
        "# Select 20 random questions for evaluation\n",
        "evaluation_indices = random.sample(range(len(questions)), 20)\n",
        "evaluation_questions = [questions[i] for i in evaluation_indices]\n",
        "\n",
        "# Step 6: Function to retrieve the 5 most similar questions based on query\n",
        "def find_similar_questions(query, top_k=5):\n",
        "    # Generate embedding for the query\n",
        "    query_embedding = generate_embedding(f\"Question: {query}\")\n",
        "\n",
        "    # Search Faiss index for nearest neighbors\n",
        "    distances, indices = faiss_index.search(query_embedding.reshape(1, -1), top_k)\n",
        "\n",
        "    # Return the most similar questions\n",
        "    closest_questions = [questions[i] for i in indices[0]]\n",
        "    return closest_questions\n",
        "\n",
        "# Function to assess similarity between two queries using Vertex AI\n",
        "def similarity_assessment_model(query1, query2):\n",
        "    generation_config = {\n",
        "        \"max_output_tokens\": 8192,\n",
        "        \"temperature\": 1,\n",
        "        \"top_p\": 0.95,\n",
        "        \"response_mime_type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    vertexai.init(project=\"monobrain-development\", location=\"us-central1\")\n",
        "    model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
        "    responses = model.generate_content([\n",
        "        \"\"\"あなたは日本語の小説の編集者をしており、文章のスペシャリストです。\n",
        "        以下の文章を比べて類似度を数値5段階で出力してください。\\n\\n\n",
        "        文章の大まかな内容と細かい内容、文章表現全てが類似している場合は : 5\n",
        "        文章の大まかな内容は類似しているが、細かい内容が違っており、文章表現が類似しているものは : 4\n",
        "        文章の大まかな内容は類似しているが、細かい内容が違っており、文章表現が異なっているものは : 3\n",
        "        文章の大まかな内容は異なっているが、文内で使われている単語が似通っている場合 : 2\n",
        "        文書の内容は間違っているが、文章表現や文章の雰囲気が似ている場合 : 1\n",
        "        文章の内容が違っており、文章表現や単語も異なっている場合 : 0\n",
        "\n",
        "        jsonの形式は{\"similarity\":3,\"comment\":\"ここになぜそのような評価にしたかのコメント\"}\n",
        "\n",
        "        文章1 :\"\"\" + query1 + \"\\n 文章2 : \" + query2],\n",
        "        generation_config=generation_config,\n",
        "        stream=False,\n",
        "    )\n",
        "\n",
        "    score = float(json.loads(responses.candidates[0].content.parts[0].text)['similarity'])\n",
        "    return score\n",
        "\n",
        "# Evaluate the system with 20 selected questions using T5 and Faiss\n",
        "similarity_scores = []\n",
        "#time\n",
        "total_time = 0.0\n",
        "\n",
        "# Iterate over each record to find the top 5 similar questions and calculate similarity scores\n",
        "for record in evaluation_questions:\n",
        "    start_time = time.time()\n",
        "    similar5data = find_similar_questions(record)\n",
        "    end_time = time.time()\n",
        "    time_for_using_rag = end_time - start_time\n",
        "    total_time += time_for_using_rag\n",
        "    for one_data in similar5data:\n",
        "        score = similarity_assessment_model(record, one_data)\n",
        "        similarity_scores.append(score)\n",
        "\n",
        "# Calculate the average similarity score\n",
        "if similarity_scores:\n",
        "    average_similarity = np.mean(similarity_scores)\n",
        "    print(f\"\\nThe average score of this model is: {average_similarity}\")\n",
        "else:\n",
        "    print(\"\\nNo similarity scores were calculated.\")\n",
        "\n",
        "# Calculate the average time taken for RAG retrieval\n",
        "average_time = total_time / len(evaluation_questions)\n",
        "print(f\"\\nThe average time taken for RAG retrieval is: {average_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ETTQ1POb-olI",
      "metadata": {
        "id": "ETTQ1POb-olI"
      },
      "source": [
        "Without Category Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "DEehCK9n_uh9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 208477,
          "status": "ok",
          "timestamp": 1733556550812,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "DEehCK9n_uh9",
        "outputId": "59dedfbe-6438-42b1-884e-fc1896a6dba8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average score of this model is: 1.57\n",
            "\n",
            "The average time taken for RAG retrieval is: 0.0609 seconds\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import faiss\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import base64\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part, SafetySetting\n",
        "\n",
        "# Step 1: Load your dataset (output_categories.jsonl)\n",
        "with jsonlines.open('qa_pairs_for_rag_1108.jsonl') as reader:\n",
        "    qa_pairs = [obj for obj in reader]\n",
        "\n",
        "# Step 2: Prepare the question-answer-category pairs as documents\n",
        "documents = [f\"Question: {pair['question']} Answer: {pair['answer']}\" for pair in qa_pairs]\n",
        "questions = [pair['question'] for pair in qa_pairs]\n",
        "answers = [pair['answer'] for pair in qa_pairs]\n",
        "\n",
        "# Initialize tokenizer and model for T5\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
        "\n",
        "# Step 3: Function to generate embeddings using T5\n",
        "def generate_embedding(text):\n",
        "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        output = model.encoder(input_ids)\n",
        "    embeddings = output.last_hidden_state.mean(dim=1).squeeze().numpy()  # Average pooling\n",
        "    return embeddings\n",
        "\n",
        "# Step 4: Generate embeddings for each question\n",
        "question_embeddings = [generate_embedding(doc) for doc in documents]\n",
        "\n",
        "# Step 5: Initialize Faiss index\n",
        "d = question_embeddings[0].shape[0]  # Dimensionality of embeddings\n",
        "faiss_index = faiss.IndexFlatL2(d)\n",
        "faiss_index.add(np.array(question_embeddings))\n",
        "\n",
        "# Select 20 random questions for evaluation\n",
        "evaluation_indices = random.sample(range(len(questions)), 20)\n",
        "evaluation_questions = [questions[i] for i in evaluation_indices]\n",
        "\n",
        "# Step 6: Function to retrieve the 5 most similar questions based on query\n",
        "def find_similar_questions(query, top_k=5):\n",
        "    # Generate embedding for the query\n",
        "    query_embedding = generate_embedding(f\"Question: {query}\")\n",
        "\n",
        "    # Search Faiss index for nearest neighbors\n",
        "    distances, indices = faiss_index.search(query_embedding.reshape(1, -1), top_k)\n",
        "\n",
        "    # Return the most similar questions\n",
        "    closest_questions = [questions[i] for i in indices[0]]\n",
        "    return closest_questions\n",
        "\n",
        "# Function to assess similarity between two queries using Vertex AI\n",
        "def similarity_assessment_model(query1, query2):\n",
        "    generation_config = {\n",
        "        \"max_output_tokens\": 8192,\n",
        "        \"temperature\": 1,\n",
        "        \"top_p\": 0.95,\n",
        "        \"response_mime_type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    vertexai.init(project=\"monobrain-development\", location=\"us-central1\")\n",
        "    model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
        "    responses = model.generate_content([\n",
        "        \"\"\"あなたは日本語の小説の編集者をしており、文章のスペシャリストです。\n",
        "        以下の文章を比べて類似度を数値5段階で出力してください。\\n\\n\n",
        "        文章の大まかな内容と細かい内容、文章表現全てが類似している場合は : 5\n",
        "        文章の大まかな内容は類似しているが、細かい内容が違っており、文章表現が類似しているものは : 4\n",
        "        文章の大まかな内容は類似しているが、細かい内容が違っており、文章表現が異なっているものは : 3\n",
        "        文章の大まかな内容は異なっているが、文内で使われている単語が似通っている場合 : 2\n",
        "        文書の内容は間違っているが、文章表現や文章の雰囲気が似ている場合 : 1\n",
        "        文章の内容が違っており、文章表現や単語も異なっている場合 : 0\n",
        "\n",
        "        jsonの形式は{\"similarity\":3,\"comment\":\"ここになぜそのような評価にしたかのコメント\"}\n",
        "\n",
        "        文章1 :\"\"\" + query1 + \"\\n 文章2 : \" + query2],\n",
        "        generation_config=generation_config,\n",
        "        stream=False,\n",
        "    )\n",
        "\n",
        "    score = float(json.loads(responses.candidates[0].content.parts[0].text)['similarity'])\n",
        "    return score\n",
        "\n",
        "# Evaluate the system with 20 selected questions using T5 and Faiss\n",
        "similarity_scores = []\n",
        "#time\n",
        "total_time = 0.0\n",
        "\n",
        "# Iterate over each record to find the top 5 similar questions and calculate similarity scores\n",
        "for record in evaluation_questions:\n",
        "    start_time = time.time()\n",
        "    similar5data = find_similar_questions(record)\n",
        "    end_time = time.time()\n",
        "    time_for_using_rag = end_time - start_time\n",
        "    total_time += time_for_using_rag\n",
        "    for one_data in similar5data:\n",
        "        score = similarity_assessment_model(record, one_data)\n",
        "        similarity_scores.append(score)\n",
        "\n",
        "# Calculate the average similarity score\n",
        "if similarity_scores:\n",
        "    average_similarity = np.mean(similarity_scores)\n",
        "    print(f\"\\nThe average score of this model is: {average_similarity}\")\n",
        "else:\n",
        "    print(\"\\nNo similarity scores were calculated.\")\n",
        "\n",
        "# Calculate the average time taken for RAG retrieval\n",
        "average_time = total_time / len(evaluation_questions)\n",
        "print(f\"\\nThe average time taken for RAG retrieval is: {average_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zXwk9FsNA1VP",
      "metadata": {
        "id": "zXwk9FsNA1VP"
      },
      "source": [
        "# **TF-IDF + Annoy (Model 3)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "5Y_XAqoY_vjD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 19456,
          "status": "ok",
          "timestamp": 1733556644227,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "5Y_XAqoY_vjD",
        "outputId": "7c81a1fa-aadc-4db6-c222-50b5794c3c0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/647.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.8/647.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn annoy jsonlines --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "G4YpMvc7-dzN",
      "metadata": {
        "id": "G4YpMvc7-dzN"
      },
      "source": [
        "With Category Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "WWvIOd3L_1lf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 56025,
          "status": "ok",
          "timestamp": 1733556867273,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "WWvIOd3L_1lf",
        "outputId": "88356612-bb44-4202-d367-8dfdfb94c1f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average score of this model is: 2.3\n",
            "\n",
            "The average time taken for RAG retrieval is: 0.0101 seconds\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from annoy import AnnoyIndex\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "import random\n",
        "import base64\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part, SafetySetting\n",
        "\n",
        "# Step 1: Load your dataset (output_categories.jsonl)\n",
        "with jsonlines.open('output_with_categories_1107.jsonl') as reader:\n",
        "    qa_pairs = [obj for obj in reader]\n",
        "\n",
        "# Step 2: Prepare the question-answer-category pairs as documents\n",
        "documents = [f\"Question: {pair['question']} Answer: {pair['answer']} Category: {pair['category']}\" for pair in qa_pairs]\n",
        "questions = [pair['question'] for pair in qa_pairs]\n",
        "answers = [pair['answer'] for pair in qa_pairs]\n",
        "categories = [pair['category'] for pair in qa_pairs]\n",
        "\n",
        "# Step 3: Initialize the TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Convert the documents to TF-IDF vectors\n",
        "tfidf_matrix = vectorizer.fit_transform(documents).toarray()  # Convert to numpy array\n",
        "\n",
        "# Step 4: Initialize the ANNoy index\n",
        "dim = tfidf_matrix.shape[1]  # Dimensionality of the vectors (number of features)\n",
        "annoy_index = AnnoyIndex(dim, 'angular')  # 'angular' is cosine similarity equivalent\n",
        "\n",
        "# Step 5: Add the TF-IDF vectors to the ANNoy index\n",
        "for i, vec in enumerate(tfidf_matrix):\n",
        "    annoy_index.add_item(i, vec)\n",
        "\n",
        "# Build the index (this step can take a while depending on the number of documents)\n",
        "annoy_index.build(10)  # 10 trees is a good trade-off for speed and accuracy\n",
        "\n",
        "# Select 20 random questions for evaluation\n",
        "evaluation_indices = random.sample(range(len(questions)), 20)\n",
        "evaluation_questions = [questions[i] for i in evaluation_indices]\n",
        "\n",
        "# Step 6: Function to retrieve the 5 most similar questions based on query\n",
        "def find_similar_questions(query, top_k=5):\n",
        "    # Convert the query to a TF-IDF vector\n",
        "    query_tfidf = vectorizer.transform([f\"Question: {query}\"]).toarray()\n",
        "\n",
        "    # Get the nearest neighbors from ANNoy\n",
        "    nearest_neighbors = annoy_index.get_nns_by_vector(query_tfidf[0], top_k, include_distances=False)\n",
        "\n",
        "    # Return the most similar questions\n",
        "    closest_questions = [questions[i] for i in nearest_neighbors]\n",
        "    return closest_questions\n",
        "\n",
        "# Function to assess similarity between two queries using Vertex AI\n",
        "def similarity_assessment_model(query1, query2):\n",
        "    generation_config = {\n",
        "        \"max_output_tokens\": 8192,\n",
        "        \"temperature\": 1,\n",
        "        \"top_p\": 0.95,\n",
        "        \"response_mime_type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    vertexai.init(project=\"monobrain-development\", location=\"us-central1\")\n",
        "    model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
        "    responses = model.generate_content([\n",
        "        \"\"\"あなたは日本語の小説の編集者をしており、文章のスペシャリストです。\n",
        "        以下の文章を比べて類似度を数値5段階で出力してください。\\n\\n\n",
        "        文章の大まかな内容と細かい内容、文章表現全てが類似している場合は : 5\n",
        "        文章の大まかな内容は類似しているが、細かい内容が違っており、文章表現が類似しているものは : 4\n",
        "        文章の大まかな内容は類似しているが、細かい内容が違っており、文章表現が異なっているものは : 3\n",
        "        文章の大まかな内容は異なっているが、文内で使われている単語が似通っている場合 : 2\n",
        "        文書の内容は間違っているが、文章表現や文章の雰囲気が似ている場合 : 1\n",
        "        文章の内容が違っており、文章表現や単語も異なっている場合 : 0\n",
        "\n",
        "        jsonの形式は{\"similarity\":3,\"comment\":\"ここになぜそのような評価にしたかのコメント\"}\n",
        "\n",
        "        文章1 :\"\"\" + query1 + \"\\n 文章2 : \" + query2],\n",
        "        generation_config=generation_config,\n",
        "        stream=False,\n",
        "    )\n",
        "\n",
        "    score = float(json.loads(responses.candidates[0].content.parts[0].text)['similarity'])\n",
        "    return score\n",
        "\n",
        "# Evaluate the system with 20 selected questions using Word2Vec\n",
        "similarity_scores = []\n",
        "#time\n",
        "total_time = 0.0\n",
        "\n",
        "# Iterate over each record to find the top 5 similar questions and calculate similarity scores\n",
        "for record in evaluation_questions:\n",
        "    start_time = time.time()\n",
        "    similar5data = find_similar_questions(record)\n",
        "    end_time = time.time()\n",
        "    time_for_using_rag = end_time - start_time\n",
        "    total_time += time_for_using_rag\n",
        "    for one_data in similar5data:\n",
        "        score = similarity_assessment_model(record, one_data)\n",
        "        similarity_scores.append(score)\n",
        "\n",
        "# Calculate the average similarity score\n",
        "if similarity_scores:\n",
        "    average_similarity = np.mean(similarity_scores)\n",
        "    print(f\"\\nThe average score of this model is: {average_similarity}\")\n",
        "else:\n",
        "    print(\"\\nNo similarity scores were calculated.\")\n",
        "\n",
        "# Calculate the average time taken for RAG retrieval\n",
        "average_time = total_time / len(evaluation_questions)\n",
        "print(f\"\\nThe average time taken for RAG retrieval is: {average_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h4fPJc5_-q7J",
      "metadata": {
        "id": "h4fPJc5_-q7J"
      },
      "source": [
        "Without Category Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "tf1L8MaqAdsx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 56464,
          "status": "ok",
          "timestamp": 1733556923732,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "tf1L8MaqAdsx",
        "outputId": "f9083195-ee57-4047-aa05-592f6ad265f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average score of this model is: 2.72\n",
            "\n",
            "The average time taken for RAG retrieval is: 0.0083 seconds\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from annoy import AnnoyIndex\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "import random\n",
        "import base64\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part, SafetySetting\n",
        "\n",
        "# Step 1: Load your dataset (output_categories.jsonl)\n",
        "with jsonlines.open('qa_pairs_for_rag_1108.jsonl') as reader:\n",
        "    qa_pairs = [obj for obj in reader]\n",
        "\n",
        "# Step 2: Prepare the question-answer-category pairs as documents\n",
        "documents = [f\"Question: {pair['question']} Answer: {pair['answer']}\" for pair in qa_pairs]\n",
        "questions = [pair['question'] for pair in qa_pairs]\n",
        "answers = [pair['answer'] for pair in qa_pairs]\n",
        "\n",
        "# Step 3: Initialize the TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Convert the documents to TF-IDF vectors\n",
        "tfidf_matrix = vectorizer.fit_transform(documents).toarray()  # Convert to numpy array\n",
        "\n",
        "# Step 4: Initialize the ANNoy index\n",
        "dim = tfidf_matrix.shape[1]  # Dimensionality of the vectors (number of features)\n",
        "annoy_index = AnnoyIndex(dim, 'angular')  # 'angular' is cosine similarity equivalent\n",
        "\n",
        "# Step 5: Add the TF-IDF vectors to the ANNoy index\n",
        "for i, vec in enumerate(tfidf_matrix):\n",
        "    annoy_index.add_item(i, vec)\n",
        "\n",
        "# Build the index (this step can take a while depending on the number of documents)\n",
        "annoy_index.build(10)  # 10 trees is a good trade-off for speed and accuracy\n",
        "\n",
        "# Select 20 random questions for evaluation\n",
        "evaluation_indices = random.sample(range(len(questions)), 20)\n",
        "evaluation_questions = [questions[i] for i in evaluation_indices]\n",
        "\n",
        "# Step 6: Function to retrieve the 5 most similar questions based on query\n",
        "def find_similar_questions(query, top_k=5):\n",
        "    # Convert the query to a TF-IDF vector\n",
        "    query_tfidf = vectorizer.transform([f\"Question: {query}\"]).toarray()\n",
        "\n",
        "    # Get the nearest neighbors from ANNoy\n",
        "    nearest_neighbors = annoy_index.get_nns_by_vector(query_tfidf[0], top_k, include_distances=False)\n",
        "\n",
        "    # Return the most similar questions\n",
        "    closest_questions = [questions[i] for i in nearest_neighbors]\n",
        "    return closest_questions\n",
        "\n",
        "# Function to assess similarity between two queries using Vertex AI\n",
        "def similarity_assessment_model(query1, query2):\n",
        "    generation_config = {\n",
        "        \"max_output_tokens\": 8192,\n",
        "        \"temperature\": 1,\n",
        "        \"top_p\": 0.95,\n",
        "        \"response_mime_type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    vertexai.init(project=\"monobrain-development\", location=\"us-central1\")\n",
        "    model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
        "    responses = model.generate_content([\n",
        "        \"\"\"あなたは日本語の小説の編集者をしており、文章のスペシャリストです。\n",
        "        以下の文章を比べて類似度を数値5段階で出力してください。\\n\\n\n",
        "        文章の大まかな内容と細かい内容、文章表現全てが類似している場合は : 5\n",
        "        文章の大まかな内容は類似しているが、細かい内容が違っており、文章表現が類似しているものは : 4\n",
        "        文章の大まかな内容は類似しているが、細かい内容が違っており、文章表現が異なっているものは : 3\n",
        "        文章の大まかな内容は異なっているが、文内で使われている単語が似通っている場合 : 2\n",
        "        文書の内容は間違っているが、文章表現や文章の雰囲気が似ている場合 : 1\n",
        "        文章の内容が違っており、文章表現や単語も異なっている場合 : 0\n",
        "\n",
        "        jsonの形式は{\"similarity\":3,\"comment\":\"ここになぜそのような評価にしたかのコメント\"}\n",
        "\n",
        "        文章1 :\"\"\" + query1 + \"\\n 文章2 : \" + query2],\n",
        "        generation_config=generation_config,\n",
        "        stream=False,\n",
        "    )\n",
        "\n",
        "    score = float(json.loads(responses.candidates[0].content.parts[0].text)['similarity'])\n",
        "    return score\n",
        "\n",
        "# Evaluate the system with 20 selected questions using Word2Vec\n",
        "similarity_scores = []\n",
        "\n",
        "#time\n",
        "total_time = 0.0\n",
        "\n",
        "# Iterate over each record to find the top 5 similar questions and calculate similarity scores\n",
        "for record in evaluation_questions:\n",
        "    start_time = time.time()\n",
        "    similar5data = find_similar_questions(record)\n",
        "    end_time = time.time()\n",
        "    time_for_using_rag = end_time - start_time\n",
        "    total_time += time_for_using_rag\n",
        "    for one_data in similar5data:\n",
        "        score = similarity_assessment_model(record, one_data)\n",
        "        similarity_scores.append(score)\n",
        "\n",
        "# Calculate the average similarity score\n",
        "if similarity_scores:\n",
        "    average_similarity = np.mean(similarity_scores)\n",
        "    print(f\"\\nThe average score of this model is: {average_similarity}\")\n",
        "else:\n",
        "    print(\"\\nNo similarity scores were calculated.\")\n",
        "\n",
        "# Calculate the average time taken for RAG retrieval\n",
        "average_time = total_time / len(evaluation_questions)\n",
        "print(f\"\\nThe average time taken for RAG retrieval is: {average_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fiFONUuiBNKb",
      "metadata": {
        "id": "fiFONUuiBNKb"
      },
      "source": [
        "# **Universal Sentence Encoder + HNSW (Model 4)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "J-JFw8cVBGG4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 54031,
          "status": "ok",
          "timestamp": 1733557023979,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "J-JFw8cVBGG4",
        "outputId": "e64b3f9f-c191-4f86-820c-fd6596d97e6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Collecting hnswlib\n",
            "  Downloading hnswlib-0.8.0.tar.gz (36 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (69.5.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.67.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (2.17.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Building wheels for collected packages: hnswlib\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.8.0-cp310-cp310-linux_x86_64.whl size=2360797 sha256=c7b8f0706e48359c0d1cff661de35a0e9bca468d2135a8947eec0e9221dfa63d\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/a9/3e/3e5d59ee41664eb31a4e6de67d1846f86d16d93c45f277c4e7\n",
            "Successfully built hnswlib\n",
            "Installing collected packages: hnswlib\n",
            "Successfully installed hnswlib-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow tensorflow-hub hnswlib numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jM-P2MiW-fBI",
      "metadata": {
        "id": "jM-P2MiW-fBI"
      },
      "source": [
        "With Category Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "QtjRAEG1CJL8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 68526,
          "status": "ok",
          "timestamp": 1733557154966,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "QtjRAEG1CJL8",
        "outputId": "8eb1fac3-14e1-40e1-dc0f-60aed8e22b8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average score of this model is: 0.68\n",
            "\n",
            "The average time taken for RAG retrieval is: 0.0043 seconds\n"
          ]
        }
      ],
      "source": [
        "import jsonlines\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import hnswlib\n",
        "import json\n",
        "import base64\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part, SafetySetting\n",
        "\n",
        "# Step 1: Load your dataset (output_categories.jsonl)\n",
        "with jsonlines.open('output_with_categories_1107.jsonl') as reader:\n",
        "    qa_pairs = [obj for obj in reader]\n",
        "\n",
        "# Step 2: Prepare the question-answer-category pairs as documents\n",
        "documents = [f\"Question: {pair['question']} Answer: {pair['answer']} Category: {pair['category']}\" for pair in qa_pairs]\n",
        "questions = [pair['question'] for pair in qa_pairs]\n",
        "answers = [pair['answer'] for pair in qa_pairs]\n",
        "categories = [pair['category'] for pair in qa_pairs]\n",
        "\n",
        "# Load Universal Sentence Encoder model from TensorFlow Hub\n",
        "use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "\n",
        "# Step 3: Encode the documents using USE\n",
        "question_embeddings = use_model(documents).numpy()\n",
        "\n",
        "# Step 4: Create HNSW index\n",
        "dim = question_embeddings.shape[1]\n",
        "p = hnswlib.Index(space='cosine', dim=dim)\n",
        "p.init_index(max_elements=len(questions), ef_construction=200, M=16)\n",
        "p.add_items(question_embeddings)\n",
        "p.set_ef(50)  # ef should be set to a value between 10 and 100 for best performance\n",
        "\n",
        "# Select 20 random questions for evaluation\n",
        "evaluation_indices = random.sample(range(len(questions)), 20)\n",
        "evaluation_questions = [questions[i] for i in evaluation_indices]\n",
        "\n",
        "# Step 5: Function to retrieve the 5 most similar questions based on query\n",
        "def find_similar_questions(query, top_k=5):\n",
        "    # Encode the query using USE\n",
        "    query_embedding = use_model([f\"Question: {query}\"]).numpy()\n",
        "\n",
        "    # Search the HNSW index\n",
        "    labels, distances = p.knn_query(query_embedding, k=top_k)\n",
        "\n",
        "    # Return the most similar questions\n",
        "    closest_questions = [questions[i] for i in labels[0]]\n",
        "    return closest_questions\n",
        "\n",
        "# Function to assess similarity between two queries using Vertex AI\n",
        "def similarity_assessment_model(query1, query2):\n",
        "    generation_config = {\n",
        "        \"max_output_tokens\": 8192,\n",
        "        \"temperature\": 1,\n",
        "        \"top_p\": 0.95,\n",
        "        \"response_mime_type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    vertexai.init(project=\"monobrain-development\", location=\"us-central1\")\n",
        "    model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
        "    responses = model.generate_content([\n",
        "        \"\"\"あなたは日本語の小説の編集者をしており、文章のスペシャリストです。\n",
        "        以下の文章を比べて類似度を数値5段階で出力してください。\\n\\n\n",
        "        文章の大まかな内容と細かい内容、文章表現全てが類似している場合は : 5\n",
        "        文章の大まかな内容は類似しているが、細かい内容が違っており、文章表現が類似しているものは : 4\n",
        "        文章の大まかな内容は類似しているが、細かい内容が違っており、文章表現が異なっているものは : 3\n",
        "        文章の大まかな内容は異なっているが、文内で使われている単語が似通っている場合 : 2\n",
        "        文書の内容は間違っているが、文章表現や文章の雰囲気が似ている場合 : 1\n",
        "        文章の内容が違っており、文章表現や単語も異なっている場合 : 0\n",
        "\n",
        "        jsonの形式は{\"similarity\":3,\"comment\":\"ここになぜそのような評価にしたかのコメント\"}\n",
        "\n",
        "        文章1 :\"\"\" + query1 + \"\\n 文章2 : \" + query2],\n",
        "        generation_config=generation_config,\n",
        "        stream=False,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response_content = responses.candidates[0].content.parts[0].text\n",
        "        score = float(json.loads(response_content)['similarity'])\n",
        "    except (json.JSONDecodeError, KeyError) as e:\n",
        "        print(f\"Error decoding JSON response: {e}\")\n",
        "        score = 0.0\n",
        "    return score\n",
        "\n",
        "# Evaluate the system with 20 selected questions using USE and HNSW\n",
        "similarity_scores = []\n",
        "\n",
        "#time\n",
        "total_time = 0.0\n",
        "\n",
        "# Iterate over each record to find the top 5 similar questions and calculate similarity scores\n",
        "for record in evaluation_questions:\n",
        "    start_time = time.time()\n",
        "    similar5data = find_similar_questions(record)\n",
        "    end_time = time.time()\n",
        "    time_for_using_rag = end_time - start_time\n",
        "    total_time += time_for_using_rag\n",
        "    for one_data in similar5data:\n",
        "        score = similarity_assessment_model(record, one_data)\n",
        "        similarity_scores.append(score)\n",
        "\n",
        "# Calculate the average similarity score\n",
        "if similarity_scores:\n",
        "    average_similarity = np.mean(similarity_scores)\n",
        "    print(f\"\\nThe average score of this model is: {average_similarity}\")\n",
        "else:\n",
        "    print(\"\\nNo similarity scores were calculated.\")\n",
        "\n",
        "# Calculate the average time taken for RAG retrieval\n",
        "average_time = total_time / len(evaluation_questions)\n",
        "print(f\"\\nThe average time taken for RAG retrieval is: {average_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e_llqXCR-s6g",
      "metadata": {
        "id": "e_llqXCR-s6g"
      },
      "source": [
        "Without Category Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a4epHzt4Apbv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 55394,
          "status": "ok",
          "timestamp": 1733557214487,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "a4epHzt4Apbv",
        "outputId": "abbcfe92-e235-4d60-9cf6-1010231d1611"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average score of this model is: 0.76\n",
            "\n",
            "The average time taken for RAG retrieval is: 0.0041 seconds\n"
          ]
        }
      ],
      "source": [
        "import jsonlines\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import hnswlib\n",
        "import json\n",
        "import base64\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part, SafetySetting\n",
        "\n",
        "# Step 1: Load your dataset (output_categories.jsonl)\n",
        "with jsonlines.open('qa_pairs_for_rag_1108.jsonl') as reader:\n",
        "    qa_pairs = [obj for obj in reader]\n",
        "\n",
        "# Step 2: Prepare the question-answer-category pairs as documents\n",
        "documents = [f\"Question: {pair['question']} Answer: {pair['answer']} \" for pair in qa_pairs]\n",
        "questions = [pair['question'] for pair in qa_pairs]\n",
        "answers = [pair['answer'] for pair in qa_pairs]\n",
        "\n",
        "# Load Universal Sentence Encoder model from TensorFlow Hub\n",
        "use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "\n",
        "# Step 3: Encode the documents using USE\n",
        "question_embeddings = use_model(documents).numpy()\n",
        "\n",
        "# Step 4: Create HNSW index\n",
        "dim = question_embeddings.shape[1]\n",
        "p = hnswlib.Index(space='cosine', dim=dim)\n",
        "p.init_index(max_elements=len(questions), ef_construction=200, M=16)\n",
        "p.add_items(question_embeddings)\n",
        "p.set_ef(50)  # ef should be set to a value between 10 and 100 for best performance\n",
        "\n",
        "# Select 20 random questions for evaluation\n",
        "evaluation_indices = random.sample(range(len(questions)), 20)\n",
        "evaluation_questions = [questions[i] for i in evaluation_indices]\n",
        "\n",
        "# Step 5: Function to retrieve the 5 most similar questions based on query\n",
        "def find_similar_questions(query, top_k=5):\n",
        "    # Encode the query using USE\n",
        "    query_embedding = use_model([f\"Question: {query}\"]).numpy()\n",
        "\n",
        "    # Search the HNSW index\n",
        "    labels, distances = p.knn_query(query_embedding, k=top_k)\n",
        "\n",
        "    # Return the most similar questions\n",
        "    closest_questions = [questions[i] for i in labels[0]]\n",
        "    return closest_questions\n",
        "\n",
        "# Function to assess similarity between two queries using Vertex AI\n",
        "def similarity_assessment_model(query1, query2):\n",
        "    generation_config = {\n",
        "        \"max_output_tokens\": 8192,\n",
        "        \"temperature\": 1,\n",
        "        \"top_p\": 0.95,\n",
        "        \"response_mime_type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    vertexai.init(project=\"monobrain-development\", location=\"us-central1\")\n",
        "    model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
        "    responses = model.generate_content([\n",
        "        \"\"\"あなたは日本語の小説の編集者をしており、文章のスペシャリストです。\n",
        "        以下の文章を比べて類似度を数値5段階で出力してください。\\n\\n\n",
        "        文章の大まかな内容と細かい内容、文章表現全てが類似している場合は : 5\n",
        "        文章の大まかな内容は類似しているが、細かい内容が違っており、文章表現が類似しているものは : 4\n",
        "        文章の大まかな内容は類似しているが、細かい内容が違っており、文章表現が異なっているものは : 3\n",
        "        文章の大まかな内容は異なっているが、文内で使われている単語が似通っている場合 : 2\n",
        "        文書の内容は間違っているが、文章表現や文章の雰囲気が似ている場合 : 1\n",
        "        文章の内容が違っており、文章表現や単語も異なっている場合 : 0\n",
        "\n",
        "        jsonの形式は{\"similarity\":3,\"comment\":\"ここになぜそのような評価にしたかのコメント\"}\n",
        "\n",
        "        文章1 :\"\"\" + query1 + \"\\n 文章2 : \" + query2],\n",
        "        generation_config=generation_config,\n",
        "        stream=False,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response_content = responses.candidates[0].content.parts[0].text\n",
        "        score = float(json.loads(response_content)['similarity'])\n",
        "    except (json.JSONDecodeError, KeyError) as e:\n",
        "        print(f\"Error decoding JSON response: {e}\")\n",
        "        score = 0.0\n",
        "    return score\n",
        "\n",
        "# Evaluate the system with 20 selected questions using USE and HNSW\n",
        "similarity_scores = []\n",
        "\n",
        "#time\n",
        "total_time = 0.0\n",
        "\n",
        "# Iterate over each record to find the top 5 similar questions and calculate similarity scores\n",
        "for record in evaluation_questions:\n",
        "    start_time = time.time()\n",
        "    similar5data = find_similar_questions(record)\n",
        "    end_time = time.time()\n",
        "    time_for_using_rag = end_time - start_time\n",
        "    total_time += time_for_using_rag\n",
        "    for one_data in similar5data:\n",
        "        score = similarity_assessment_model(record, one_data)\n",
        "        similarity_scores.append(score)\n",
        "\n",
        "# Calculate the average similarity score\n",
        "if similarity_scores:\n",
        "    average_similarity = np.mean(similarity_scores)\n",
        "    print(f\"\\nThe average score of this model is: {average_similarity}\")\n",
        "else:\n",
        "    print(\"\\nNo similarity scores were calculated.\")\n",
        "\n",
        "# Calculate the average time taken for RAG retrieval\n",
        "average_time = total_time / len(evaluation_questions)\n",
        "print(f\"\\nThe average time taken for RAG retrieval is: {average_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ukLQ1sEtCnbO",
      "metadata": {
        "id": "ukLQ1sEtCnbO"
      },
      "source": [
        "# **Word2Vec Implementation (Model 5)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "ql5fzTZa_zIk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 41977,
          "status": "ok",
          "timestamp": 1733557273862,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "ql5fzTZa_zIk",
        "outputId": "3f73fe86-2539-459b-df30-aa453276f5fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (4.0.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (24.2.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Collecting janome\n",
            "  Downloading Janome-0.5.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: janome\n",
            "Successfully installed janome-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install jsonlines\n",
        "!pip install nltk\n",
        "!pip install gensim\n",
        "!pip install janome"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UPsZ0jqX-gsp",
      "metadata": {
        "id": "UPsZ0jqX-gsp"
      },
      "source": [
        "With Category Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "oYwxN4HnFz9E",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 117945,
          "status": "ok",
          "timestamp": 1733557391804,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "oYwxN4HnFz9E",
        "outputId": "3b2121f2-6596-4aed-cbd6-dbbf769f6e87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average score of this model is: 0.89\n",
            "\n",
            "The average time taken for RAG retrieval is: 1.4757 seconds\n"
          ]
        }
      ],
      "source": [
        "import jsonlines\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from janome.tokenizer import Tokenizer  # For Japanese tokenization\n",
        "import base64\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part, SafetySetting\n",
        "\n",
        "# Initialize Janome tokenizer for Japanese\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Function to tokenize Japanese text\n",
        "def tokenize_japanese(text):\n",
        "    return [token.surface for token in tokenizer.tokenize(text)]\n",
        "\n",
        "# Step 1: Load your dataset (output_categories.jsonl)\n",
        "with jsonlines.open('output_with_categories_1107.jsonl') as reader:\n",
        "    qa_pairs = [obj for obj in reader]\n",
        "\n",
        "# Step 2: Prepare the question-answer-category pairs as documents\n",
        "documents = [f\"Question: {pair['question']} Answer: {pair['answer']} Category: {pair['category']}\" for pair in qa_pairs]\n",
        "questions = [pair['question'] for pair in qa_pairs]\n",
        "answers = [pair['answer'] for pair in qa_pairs]\n",
        "categories = [pair['category'] for pair in qa_pairs]\n",
        "\n",
        "# Tokenize data using Janome for Japanese text\n",
        "tokenized_data = []\n",
        "for item in qa_pairs:\n",
        "    question_tokens = tokenize_japanese(item['question'])\n",
        "    answer_tokens = tokenize_japanese(item['answer'])\n",
        "    category_tokens = tokenize_japanese(item['category'])\n",
        "    # Flatten question, answer, and category into the same list for Word2Vec training\n",
        "    tokenized_data.append(question_tokens + answer_tokens + category_tokens)\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Create a vectorized representation of questions without averaging word vectors\n",
        "def get_combined_vector(tokens):\n",
        "    vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
        "    if vectors:\n",
        "        return np.vstack(vectors)  # Stack vectors vertically\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "question_vectors = []\n",
        "for item in qa_pairs:\n",
        "    question_tokens = tokenize_japanese(item['question'])\n",
        "    question_vec = get_combined_vector(question_tokens)\n",
        "    question_vectors.append(question_vec)\n",
        "\n",
        "# Select 20 random questions for evaluation\n",
        "evaluation_indices = random.sample(range(len(questions)), 20)\n",
        "evaluation_questions = [questions[i] for i in evaluation_indices]\n",
        "\n",
        "# Function to find the 5 most similar questions based on query\n",
        "def find_similar_questions(query, top_k=5):\n",
        "    query_tokens = tokenize_japanese(query)\n",
        "    query_vec = get_combined_vector(query_tokens)\n",
        "\n",
        "    if query_vec is not None:\n",
        "        valid_vectors = [(i, vec) for i, vec in enumerate(question_vectors) if vec is not None]\n",
        "        similarities = []\n",
        "        for i, vec in valid_vectors:\n",
        "            similarity = cosine_similarity(query_vec, vec).mean()  # Calculate similarity between stacked vectors\n",
        "            similarities.append((i, similarity))\n",
        "        similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
        "        most_similar_indices = [i for i, _ in similarities[:top_k]]\n",
        "        closest_questions = [questions[i] for i in most_similar_indices]\n",
        "        return closest_questions\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "# Function to assess similarity between two queries using Vertex AI\n",
        "def similarity_assessment_model(query1, query2):\n",
        "    generation_config = {\n",
        "        \"max_output_tokens\": 8192,\n",
        "        \"temperature\": 1,\n",
        "        \"top_p\": 0.95,\n",
        "        \"response_mime_type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    vertexai.init(project=\"monobrain-development\", location=\"us-central1\")\n",
        "    model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
        "    responses = model.generate_content([\n",
        "        \"\"\"あなたは日本語の小説の編集者をしており、文章のスペシャリストです。\n",
        "        以下の文章を比べて類似度を数値5段階で出力してください。\\n\\n\n",
        "        文章の大まかな内容と細かい内容、文章表現全てが類似している場合は : 5\n",
        "        文章の大まかな内容は類似しているが、細かい内容が違っており、文章表現が類似しているものは : 4\n",
        "        文章の大まかな内容は類似しているが、細かい内容が違っており、文章表現が異なっているものは : 3\n",
        "        文章の大まかな内容は異なっているが、文内で使われている単語が似通っている場合 : 2\n",
        "        文書の内容は間違っているが、文章表現や文章の雰囲気が似ている場合 : 1\n",
        "        文章の内容が違っており、文章表現や単語も異なっている場合 : 0\n",
        "\n",
        "        jsonの形式は{\"similarity\":3,\"comment\":\"ここになぜそのような評価にしたかのコメント\"}\n",
        "\n",
        "        文章1 :\"\"\" + query1 + \"\\n 文章2 : \" + query2],\n",
        "        generation_config=generation_config,\n",
        "        stream=False,\n",
        "    )\n",
        "\n",
        "    score = float(json.loads(responses.candidates[0].content.parts[0].text)['similarity'])\n",
        "    return score\n",
        "\n",
        "# Evaluate the system with 20 selected questions using Word2Vec\n",
        "similarity_scores = []\n",
        "#time\n",
        "total_time = 0.0\n",
        "\n",
        "# Iterate over each record to find the top 5 similar questions and calculate similarity scores\n",
        "for record in evaluation_questions:\n",
        "    start_time = time.time()\n",
        "    similar5data = find_similar_questions(record)\n",
        "    end_time = time.time()\n",
        "    time_for_using_rag = end_time - start_time\n",
        "    total_time += time_for_using_rag\n",
        "    for one_data in similar5data:\n",
        "        score = similarity_assessment_model(record, one_data)\n",
        "        similarity_scores.append(score)\n",
        "\n",
        "# Calculate the average similarity score\n",
        "if similarity_scores:\n",
        "    average_similarity = np.mean(similarity_scores)\n",
        "    print(f\"\\nThe average score of this model is: {average_similarity}\")\n",
        "else:\n",
        "    print(\"\\nNo similarity scores were calculated.\")\n",
        "\n",
        "# Calculate the average time taken for RAG retrieval\n",
        "average_time = total_time / len(evaluation_questions)\n",
        "print(f\"\\nThe average time taken for RAG retrieval is: {average_time:.4f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e4orB1M-uu2",
      "metadata": {
        "id": "3e4orB1M-uu2"
      },
      "source": [
        "Without Category Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "QkxTnkOsAzOa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 114508,
          "status": "ok",
          "timestamp": 1733557508035,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "QkxTnkOsAzOa",
        "outputId": "ef3cbf65-526a-412f-ae53-c512412ad80a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average score of this model is: 1.01\n",
            "\n",
            "The average time taken for RAG retrieval is: 1.4214 seconds\n"
          ]
        }
      ],
      "source": [
        "import jsonlines\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from janome.tokenizer import Tokenizer  # For Japanese tokenization\n",
        "import base64\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part, SafetySetting\n",
        "\n",
        "# Initialize Janome tokenizer for Japanese\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Function to tokenize Japanese text\n",
        "def tokenize_japanese(text):\n",
        "    return [token.surface for token in tokenizer.tokenize(text)]\n",
        "\n",
        "# Step 1: Load your dataset (output_categories.jsonl)\n",
        "with jsonlines.open('qa_pairs_for_rag_1108.jsonl') as reader:\n",
        "    qa_pairs = [obj for obj in reader]\n",
        "\n",
        "# Step 2: Prepare the question-answer-category pairs as documents\n",
        "documents = [f\"Question: {pair['question']} Answer: {pair['answer']}\" for pair in qa_pairs]\n",
        "questions = [pair['question'] for pair in qa_pairs]\n",
        "answers = [pair['answer'] for pair in qa_pairs]\n",
        "\n",
        "# Tokenize data using Janome for Japanese text\n",
        "tokenized_data = []\n",
        "for item in qa_pairs:\n",
        "    question_tokens = tokenize_japanese(item['question'])\n",
        "    answer_tokens = tokenize_japanese(item['answer'])\n",
        "    # Flatten question, answer, and category into the same list for Word2Vec training\n",
        "    tokenized_data.append(question_tokens + answer_tokens)\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Create a vectorized representation of questions without averaging word vectors\n",
        "def get_combined_vector(tokens):\n",
        "    vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
        "    if vectors:\n",
        "        return np.vstack(vectors)  # Stack vectors vertically\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "question_vectors = []\n",
        "for item in qa_pairs:\n",
        "    question_tokens = tokenize_japanese(item['question'])\n",
        "    question_vec = get_combined_vector(question_tokens)\n",
        "    question_vectors.append(question_vec)\n",
        "\n",
        "# Select 20 random questions for evaluation\n",
        "evaluation_indices = random.sample(range(len(questions)), 20)\n",
        "evaluation_questions = [questions[i] for i in evaluation_indices]\n",
        "\n",
        "# Function to find the 5 most similar questions based on query\n",
        "def find_similar_questions(query, top_k=5):\n",
        "    query_tokens = tokenize_japanese(query)\n",
        "    query_vec = get_combined_vector(query_tokens)\n",
        "\n",
        "    if query_vec is not None:\n",
        "        valid_vectors = [(i, vec) for i, vec in enumerate(question_vectors) if vec is not None]\n",
        "        similarities = []\n",
        "        for i, vec in valid_vectors:\n",
        "            similarity = cosine_similarity(query_vec, vec).mean()  # Calculate similarity between stacked vectors\n",
        "            similarities.append((i, similarity))\n",
        "        similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
        "        most_similar_indices = [i for i, _ in similarities[:top_k]]\n",
        "        closest_questions = [questions[i] for i in most_similar_indices]\n",
        "        return closest_questions\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "# Function to assess similarity between two queries using Vertex AI\n",
        "def similarity_assessment_model(query1, query2):\n",
        "    generation_config = {\n",
        "        \"max_output_tokens\": 8192,\n",
        "        \"temperature\": 1,\n",
        "        \"top_p\": 0.95,\n",
        "        \"response_mime_type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    vertexai.init(project=\"monobrain-development\", location=\"us-central1\")\n",
        "    model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
        "    responses = model.generate_content([\n",
        "        \"\"\"あなたは日本語の小説の編集者をしており、文章のスペシャリストです。\n",
        "        以下の文章を比べて類似度を数値5段階で出力してください。\\n\\n\n",
        "        文章の大まかな内容と細かい内容、文章表現全てが類似している場合は : 5\n",
        "        文章の大まかな内容は類似しているが、細かい内容が違っており、文章表現が類似しているものは : 4\n",
        "        文章の大まかな内容は類似しているが、細かい内容が違っており、文章表現が異なっているものは : 3\n",
        "        文章の大まかな内容は異なっているが、文内で使われている単語が似通っている場合 : 2\n",
        "        文書の内容は間違っているが、文章表現や文章の雰囲気が似ている場合 : 1\n",
        "        文章の内容が違っており、文章表現や単語も異なっている場合 : 0\n",
        "\n",
        "        jsonの形式は{\"similarity\":3,\"comment\":\"ここになぜそのような評価にしたかのコメント\"}\n",
        "\n",
        "        文章1 :\"\"\" + query1 + \"\\n 文章2 : \" + query2],\n",
        "        generation_config=generation_config,\n",
        "        stream=False,\n",
        "    )\n",
        "\n",
        "    score = float(json.loads(responses.candidates[0].content.parts[0].text)['similarity'])\n",
        "    return score\n",
        "\n",
        "# Evaluate the system with 20 selected questions using Word2Vec\n",
        "similarity_scores = []\n",
        "#time\n",
        "total_time = 0.0\n",
        "\n",
        "# Iterate over each record to find the top 5 similar questions and calculate similarity scores\n",
        "for record in evaluation_questions:\n",
        "    start_time = time.time()\n",
        "    similar5data = find_similar_questions(record)\n",
        "    end_time = time.time()\n",
        "    time_for_using_rag = end_time - start_time\n",
        "    total_time += time_for_using_rag\n",
        "    for one_data in similar5data:\n",
        "        score = similarity_assessment_model(record, one_data)\n",
        "        similarity_scores.append(score)\n",
        "\n",
        "# Calculate the average similarity score\n",
        "if similarity_scores:\n",
        "    average_similarity = np.mean(similarity_scores)\n",
        "    print(f\"\\nThe average score of this model is: {average_similarity}\")\n",
        "else:\n",
        "    print(\"\\nNo similarity scores were calculated.\")\n",
        "\n",
        "# Calculate the average time taken for RAG retrieval\n",
        "average_time = total_time / len(evaluation_questions)\n",
        "print(f\"\\nThe average time taken for RAG retrieval is: {average_time:.4f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q6RfHs473Ppj",
      "metadata": {
        "id": "Q6RfHs473Ppj"
      },
      "source": [
        "# **Agent Builder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ZucZ3Sfr4ytf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "executionInfo": {
          "elapsed": 3789,
          "status": "ok",
          "timestamp": 1733557511820,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "ZucZ3Sfr4ytf",
        "outputId": "821fefa5-b85b-4197-b338-d4c80843c245"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting google-cloud-discoveryengine\n",
            "  Downloading google_cloud_discoveryengine-0.13.4-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-discoveryengine) (2.19.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-discoveryengine) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-discoveryengine) (1.25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-discoveryengine) (4.25.5)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-discoveryengine) (1.65.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-discoveryengine) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-discoveryengine) (1.67.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-discoveryengine) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-discoveryengine) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-discoveryengine) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-discoveryengine) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-discoveryengine) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-discoveryengine) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-discoveryengine) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-discoveryengine) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-discoveryengine) (2024.8.30)\n",
            "Downloading google_cloud_discoveryengine-0.13.4-py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-cloud-discoveryengine\n",
            "Successfully installed google-cloud-discoveryengine-0.13.4\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "4d7b90b7d13d4817bf0f4e3ceb0505e3",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install google-cloud-discoveryengine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "-lCqhLAlSyP0",
      "metadata": {
        "executionInfo": {
          "elapsed": 2667,
          "status": "ok",
          "timestamp": 1733557570789,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "-lCqhLAlSyP0"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "from google.api_core.client_options import ClientOptions\n",
        "from google.cloud import discoveryengine_v1 as discoveryengine\n",
        "\n",
        "# TODO(developer): Uncomment these variables before running the sample.\n",
        "# project_id = \"YOUR_PROJECT_ID\"\n",
        "# location = \"YOUR_LOCATION\"          # Values: \"global\", \"us\", \"eu\"\n",
        "# engine_id = \"YOUR_APP_ID\"\n",
        "# search_query = \"YOUR_SEARCH_QUERY\"\n",
        "\n",
        "\n",
        "def search_sample(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    engine_id: str,\n",
        "    search_query: str,\n",
        ") -> List[discoveryengine.SearchResponse]:\n",
        "    #  For more information, refer to:\n",
        "    # https://cloud.google.com/generative-ai-app-builder/docs/locations#specify_a_multi-region_for_your_data_store\n",
        "    client_options = (\n",
        "        ClientOptions(api_endpoint=f\"{location}-discoveryengine.googleapis.com\")\n",
        "        if location != \"global\"\n",
        "        else None\n",
        "    )\n",
        "\n",
        "    # Create a client\n",
        "    client = discoveryengine.SearchServiceClient(client_options=client_options)\n",
        "\n",
        "    # The full resource name of the search app serving config\n",
        "    serving_config = f\"projects/{project_id}/locations/{location}/collections/default_collection/engines/{engine_id}/servingConfigs/default_config\"\n",
        "\n",
        "    # Optional - only supported for unstructured data: Configuration options for search.\n",
        "    # Refer to the `ContentSearchSpec` reference for all supported fields:\n",
        "    # https://cloud.google.com/python/docs/reference/discoveryengine/latest/google.cloud.discoveryengine_v1.types.SearchRequest.ContentSearchSpec\n",
        "    content_search_spec = discoveryengine.SearchRequest.ContentSearchSpec(\n",
        "        # For information about snippets, refer to:\n",
        "        # https://cloud.google.com/generative-ai-app-builder/docs/snippets\n",
        "        snippet_spec=discoveryengine.SearchRequest.ContentSearchSpec.SnippetSpec(\n",
        "            return_snippet=True\n",
        "        ),\n",
        "        # For information about search summaries, refer to:\n",
        "        # https://cloud.google.com/generative-ai-app-builder/docs/get-search-summaries\n",
        "        summary_spec=discoveryengine.SearchRequest.ContentSearchSpec.SummarySpec(\n",
        "            summary_result_count=5,\n",
        "            include_citations=True,\n",
        "            ignore_adversarial_query=True,\n",
        "            ignore_non_summary_seeking_query=True,\n",
        "            model_prompt_spec=discoveryengine.SearchRequest.ContentSearchSpec.SummarySpec.ModelPromptSpec(\n",
        "                preamble=\"YOUR_CUSTOM_PROMPT\"\n",
        "            ),\n",
        "            model_spec=discoveryengine.SearchRequest.ContentSearchSpec.SummarySpec.ModelSpec(\n",
        "                version=\"stable\",\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Refer to the `SearchRequest` reference for all supported fields:\n",
        "    # https://cloud.google.com/python/docs/reference/discoveryengine/latest/google.cloud.discoveryengine_v1.types.SearchRequest\n",
        "    request = discoveryengine.SearchRequest(\n",
        "        serving_config=serving_config,\n",
        "        query=search_query,\n",
        "        page_size=10,\n",
        "        content_search_spec=content_search_spec,\n",
        "        query_expansion_spec=discoveryengine.SearchRequest.QueryExpansionSpec(\n",
        "            condition=discoveryengine.SearchRequest.QueryExpansionSpec.Condition.AUTO,\n",
        "        ),\n",
        "        spell_correction_spec=discoveryengine.SearchRequest.SpellCorrectionSpec(\n",
        "            mode=discoveryengine.SearchRequest.SpellCorrectionSpec.Mode.AUTO\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    response = client.search(request)\n",
        "    #print(response)\n",
        "\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "BxPXEliOS5Ax",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3599,
          "status": "ok",
          "timestamp": 1733557578723,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "BxPXEliOS5Ax",
        "outputId": "e5451abc-bbee-4581-c020-e0778baa519f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SearchPager<results {\n",
            "  id: \"e36874abcb88c00019bbf57dc8a4ae00\"\n",
            "  document {\n",
            "    struct_data {\n",
            "      fields {\n",
            "        key: \"question\"\n",
            "        value {\n",
            "          string_value: \"今日は何も取り組めませんでした。\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"id\"\n",
            "        value {\n",
            "          string_value: \"qa_433\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"category\"\n",
            "        value {\n",
            "          string_value: \"今日は取り組めず、連絡。\\n\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"answer\"\n",
            "        value {\n",
            "          string_value: \"ご報告いただきありがとうございます♪\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    name: \"projects/954273464710/locations/global/collections/default_collection/dataStores/category_1732854489356/branches/0/documents/e36874abcb88c00019bbf57dc8a4ae00\"\n",
            "    id: \"e36874abcb88c00019bbf57dc8a4ae00\"\n",
            "    derived_struct_data {\n",
            "      fields {\n",
            "        key: \"snippets\"\n",
            "        value {\n",
            "          list_value {\n",
            "            values {\n",
            "              struct_value {\n",
            "                fields {\n",
            "                  key: \"snippet\"\n",
            "                  value {\n",
            "                    string_value: \"No snippet is available for this page.\"\n",
            "                  }\n",
            "                }\n",
            "                fields {\n",
            "                  key: \"snippet_status\"\n",
            "                  value {\n",
            "                    string_value: \"NO_SNIPPET_AVAILABLE\"\n",
            "                  }\n",
            "                }\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "results {\n",
            "  id: \"7e0390da41090783733bfa101510b0f0\"\n",
            "  document {\n",
            "    struct_data {\n",
            "      fields {\n",
            "        key: \"question\"\n",
            "        value {\n",
            "          string_value: \"谷津さま今日の取り組み今日は、病院だった為に制作出来ませんでした。明日からまた取り組みます！\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"id\"\n",
            "        value {\n",
            "          string_value: \"qa_588\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"category\"\n",
            "        value {\n",
            "          string_value: \"[病院のために今日は取り組めませんでした。明日から再開します。] (17文字)\\n目的: 制作物が作れなかったことの連絡と再開時期の報告\\n\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"answer\"\n",
            "        value {\n",
            "          string_value: \"ご連絡ありがとうございます◎今日はゆっくりお子さんとの時間にあててくださいね😌\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    name: \"projects/954273464710/locations/global/collections/default_collection/dataStores/category_1732854489356/branches/0/documents/7e0390da41090783733bfa101510b0f0\"\n",
            "    id: \"7e0390da41090783733bfa101510b0f0\"\n",
            "    derived_struct_data {\n",
            "      fields {\n",
            "        key: \"snippets\"\n",
            "        value {\n",
            "          list_value {\n",
            "            values {\n",
            "              struct_value {\n",
            "                fields {\n",
            "                  key: \"snippet\"\n",
            "                  value {\n",
            "                    string_value: \"No snippet is available for this page.\"\n",
            "                  }\n",
            "                }\n",
            "                fields {\n",
            "                  key: \"snippet_status\"\n",
            "                  value {\n",
            "                    string_value: \"NO_SNIPPET_AVAILABLE\"\n",
            "                  }\n",
            "                }\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "results {\n",
            "  id: \"c72af80c21313c66b3acaf73b59d9b2f\"\n",
            "  document {\n",
            "    struct_data {\n",
            "      fields {\n",
            "        key: \"question\"\n",
            "        value {\n",
            "          string_value: \"☁️日次報告☁️今日は取り組めませんでした💦\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"id\"\n",
            "        value {\n",
            "          string_value: \"qa_021\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"category\"\n",
            "        value {\n",
            "          string_value: \"今日はオンライン授業に参加できませんでした。\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"answer\"\n",
            "        value {\n",
            "          string_value: \"ご報告ありがとうございます✨無理なく進めていきましょう☺\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    name: \"projects/954273464710/locations/global/collections/default_collection/dataStores/category_1732854489356/branches/0/documents/c72af80c21313c66b3acaf73b59d9b2f\"\n",
            "    id: \"c72af80c21313c66b3acaf73b59d9b2f\"\n",
            "    derived_struct_data {\n",
            "      fields {\n",
            "        key: \"snippets\"\n",
            "        value {\n",
            "          list_value {\n",
            "            values {\n",
            "              struct_value {\n",
            "                fields {\n",
            "                  key: \"snippet\"\n",
            "                  value {\n",
            "                    string_value: \"No snippet is available for this page.\"\n",
            "                  }\n",
            "                }\n",
            "                fields {\n",
            "                  key: \"snippet_status\"\n",
            "                  value {\n",
            "                    string_value: \"NO_SNIPPET_AVAILABLE\"\n",
            "                  }\n",
            "                }\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "results {\n",
            "  id: \"38901678e0c12d8edfb1f8de054973d5\"\n",
            "  document {\n",
            "    struct_data {\n",
            "      fields {\n",
            "        key: \"question\"\n",
            "        value {\n",
            "          string_value: \"こんばんは、今日も体調が良くなく、進められませんでした·····明日は午後お休みなので、課題をやってみようと思います！\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"id\"\n",
            "        value {\n",
            "          string_value: \"qa_1221\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"category\"\n",
            "        value {\n",
            "          string_value: \"体調不良で課題が進められなかった。明日は午後休みなので課題に取り組む予定。\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"answer\"\n",
            "        value {\n",
            "          string_value: \"体調復活されたらまた進めてくださいね💪本日もご無理なく💦\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    name: \"projects/954273464710/locations/global/collections/default_collection/dataStores/category_1732854489356/branches/0/documents/38901678e0c12d8edfb1f8de054973d5\"\n",
            "    id: \"38901678e0c12d8edfb1f8de054973d5\"\n",
            "    derived_struct_data {\n",
            "      fields {\n",
            "        key: \"snippets\"\n",
            "        value {\n",
            "          list_value {\n",
            "            values {\n",
            "              struct_value {\n",
            "                fields {\n",
            "                  key: \"snippet\"\n",
            "                  value {\n",
            "                    string_value: \"No snippet is available for this page.\"\n",
            "                  }\n",
            "                }\n",
            "                fields {\n",
            "                  key: \"snippet_status\"\n",
            "                  value {\n",
            "                    string_value: \"NO_SNIPPET_AVAILABLE\"\n",
            "                  }\n",
            "                }\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "results {\n",
            "  id: \"57369d0204541707a7e06f7efe0cf5ce\"\n",
            "  document {\n",
            "    struct_data {\n",
            "      fields {\n",
            "        key: \"question\"\n",
            "        value {\n",
            "          string_value: \"藤川さん今日は3-5実践課題再提出しました。土日はあまり課題進められませんでした。\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"id\"\n",
            "        value {\n",
            "          string_value: \"qa_1061\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"category\"\n",
            "        value {\n",
            "          string_value: \"時間外に課題を提出したことを報告。土日はあまり進められなかった。\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"answer\"\n",
            "        value {\n",
            "          string_value: \"ご報告ありがとうございます！お取り組みお疲れ様でした！進められないときもありますよね😊お取り組みできる日に対応するで無理なくメリハリつけていただければ大丈夫です💪引き続きご自身の良いペースで進めていきましょうね＾＾\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    name: \"projects/954273464710/locations/global/collections/default_collection/dataStores/category_1732854489356/branches/0/documents/57369d0204541707a7e06f7efe0cf5ce\"\n",
            "    id: \"57369d0204541707a7e06f7efe0cf5ce\"\n",
            "    derived_struct_data {\n",
            "      fields {\n",
            "        key: \"snippets\"\n",
            "        value {\n",
            "          list_value {\n",
            "            values {\n",
            "              struct_value {\n",
            "                fields {\n",
            "                  key: \"snippet\"\n",
            "                  value {\n",
            "                    string_value: \"No snippet is available for this page.\"\n",
            "                  }\n",
            "                }\n",
            "                fields {\n",
            "                  key: \"snippet_status\"\n",
            "                  value {\n",
            "                    string_value: \"NO_SNIPPET_AVAILABLE\"\n",
            "                  }\n",
            "                }\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "results {\n",
            "  id: \"4b15f40d8cdca1abedda163416b5ce5f\"\n",
            "  document {\n",
            "    struct_data {\n",
            "      fields {\n",
            "        key: \"question\"\n",
            "        value {\n",
            "          string_value: \"谷津さま体調崩しており、学習ができていませんでした…今日はFigmaと仲良くなろう会に参加しました！\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"id\"\n",
            "        value {\n",
            "          string_value: \"qa_1331\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"category\"\n",
            "        value {\n",
            "          string_value: \"体調不良のため、学習ができていませんでした。今日はFigmaと仲良くなろう会に参加しました。\\n\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"answer\"\n",
            "        value {\n",
            "          string_value: \"お取り組みお疲れ様でした！体調不良の方、すごく増えております💦一気に寒くなってきましたので、どうぞ暖かくされて体調第一にお過ごしくださいね☺️\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    name: \"projects/954273464710/locations/global/collections/default_collection/dataStores/category_1732854489356/branches/0/documents/4b15f40d8cdca1abedda163416b5ce5f\"\n",
            "    id: \"4b15f40d8cdca1abedda163416b5ce5f\"\n",
            "    derived_struct_data {\n",
            "      fields {\n",
            "        key: \"snippets\"\n",
            "        value {\n",
            "          list_value {\n",
            "            values {\n",
            "              struct_value {\n",
            "                fields {\n",
            "                  key: \"snippet\"\n",
            "                  value {\n",
            "                    string_value: \"No snippet is available for this page.\"\n",
            "                  }\n",
            "                }\n",
            "                fields {\n",
            "                  key: \"snippet_status\"\n",
            "                  value {\n",
            "                    string_value: \"NO_SNIPPET_AVAILABLE\"\n",
            "                  }\n",
            "                }\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "results {\n",
            "  id: \"40af71a2fde532f272be65712a8c6b56\"\n",
            "  document {\n",
            "    struct_data {\n",
            "      fields {\n",
            "        key: \"question\"\n",
            "        value {\n",
            "          string_value: \"☁️日次報告☁️今日は取り組めませんでした💦\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"id\"\n",
            "        value {\n",
            "          string_value: \"qa_001\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"category\"\n",
            "        value {\n",
            "          string_value: \"未完了の報告。\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"answer\"\n",
            "        value {\n",
            "          string_value: \"ご報告ありがとうございます✨無理なく進めていきましょう☺\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    name: \"projects/954273464710/locations/global/collections/default_collection/dataStores/category_1732854489356/branches/0/documents/40af71a2fde532f272be65712a8c6b56\"\n",
            "    id: \"40af71a2fde532f272be65712a8c6b56\"\n",
            "    derived_struct_data {\n",
            "      fields {\n",
            "        key: \"snippets\"\n",
            "        value {\n",
            "          list_value {\n",
            "            values {\n",
            "              struct_value {\n",
            "                fields {\n",
            "                  key: \"snippet\"\n",
            "                  value {\n",
            "                    string_value: \"No snippet is available for this page.\"\n",
            "                  }\n",
            "                }\n",
            "                fields {\n",
            "                  key: \"snippet_status\"\n",
            "                  value {\n",
            "                    string_value: \"NO_SNIPPET_AVAILABLE\"\n",
            "                  }\n",
            "                }\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "results {\n",
            "  id: \"f22d8ef04e3fcde53f1aa27dacbf906e\"\n",
            "  document {\n",
            "    struct_data {\n",
            "      fields {\n",
            "        key: \"question\"\n",
            "        value {\n",
            "          string_value: \"谷津さま今日の取り組み今日は、病院だった為に制作出来ませんでした。明日からまた取り組みます！\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"id\"\n",
            "        value {\n",
            "          string_value: \"qa_267\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"category\"\n",
            "        value {\n",
            "          string_value: \"病院のため、今日の制作はできませんでした。明日から再開します。 \\n\\n**目的:** 制作の進捗状況報告 \\n\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"answer\"\n",
            "        value {\n",
            "          string_value: \"ご連絡ありがとうございます◎今日はゆっくりお子さんとの時間にあててくださいね😌\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    name: \"projects/954273464710/locations/global/collections/default_collection/dataStores/category_1732854489356/branches/0/documents/f22d8ef04e3fcde53f1aa27dacbf906e\"\n",
            "    id: \"f22d8ef04e3fcde53f1aa27dacbf906e\"\n",
            "    derived_struct_data {\n",
            "      fields {\n",
            "        key: \"snippets\"\n",
            "        value {\n",
            "          list_value {\n",
            "            values {\n",
            "              struct_value {\n",
            "                fields {\n",
            "                  key: \"snippet\"\n",
            "                  value {\n",
            "                    string_value: \"No snippet is available for this page.\"\n",
            "                  }\n",
            "                }\n",
            "                fields {\n",
            "                  key: \"snippet_status\"\n",
            "                  value {\n",
            "                    string_value: \"NO_SNIPPET_AVAILABLE\"\n",
            "                  }\n",
            "                }\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "results {\n",
            "  id: \"2c2de9460d290275197a9f78456e9f10\"\n",
            "  document {\n",
            "    struct_data {\n",
            "      fields {\n",
            "        key: \"question\"\n",
            "        value {\n",
            "          string_value: \"谷津さんお久しぶりです1週間程体調を崩してしまい、あまり進められませんでした。取り組みもご報告できておらず、すみませんでした。本日からまた頑張っていきます！取り組んだこと10/4・2-2 動画視聴10/5・2-2 実践課題実施10/6・2-2 実践課題提出・2-3 動画視聴10/8・勉強会・2-3 実践課題実施、提出10/14・2-3 実践課題実施、再提出・3-1 動画視聴、実践課題提出・3-2 動画視聴、実践課題提出・3-3 動画視聴、実践課題実施\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"id\"\n",
            "        value {\n",
            "          string_value: \"qa_1341\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"category\"\n",
            "        value {\n",
            "          string_value: \"健康不良のため10/4-10/8は学習進捗が遅れ、10/14に学習を再開し、2-3と3-1, 3-2, 3-3の内容を学習したことを報告しています。\\n\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"answer\"\n",
            "        value {\n",
            "          string_value: \"ご報告ありがとうございます！体調不良の中でも、コツコツ進めてくださり素晴らしいです🥲体調第一で、引き続きご自身の良いペースで無理はしすぎず進めていきましょう😊お取り組みお疲れ様でした👏✨\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    name: \"projects/954273464710/locations/global/collections/default_collection/dataStores/category_1732854489356/branches/0/documents/2c2de9460d290275197a9f78456e9f10\"\n",
            "    id: \"2c2de9460d290275197a9f78456e9f10\"\n",
            "    derived_struct_data {\n",
            "      fields {\n",
            "        key: \"snippets\"\n",
            "        value {\n",
            "          list_value {\n",
            "            values {\n",
            "              struct_value {\n",
            "                fields {\n",
            "                  key: \"snippet\"\n",
            "                  value {\n",
            "                    string_value: \"No snippet is available for this page.\"\n",
            "                  }\n",
            "                }\n",
            "                fields {\n",
            "                  key: \"snippet_status\"\n",
            "                  value {\n",
            "                    string_value: \"NO_SNIPPET_AVAILABLE\"\n",
            "                  }\n",
            "                }\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "results {\n",
            "  id: \"56358b1f5f740645e4ecb88e80188ee1\"\n",
            "  document {\n",
            "    struct_data {\n",
            "      fields {\n",
            "        key: \"question\"\n",
            "        value {\n",
            "          string_value: \"今日は残業で疲れてしまったので課題進められませんでした(emoji)\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"id\"\n",
            "        value {\n",
            "          string_value: \"qa_1207\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"category\"\n",
            "        value {\n",
            "          string_value: \"残業で疲れて、課題が進められませんでした。\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"answer\"\n",
            "        value {\n",
            "          string_value: \"ご報告ありがとうございます！残業お疲れ様でした＞＜またお取り組みできる日に対応するで無理なくメリハリつけていただければ大丈夫ですので、体調第一で引き続きご自身の良いペースで進めていきましょうね＾＾\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    name: \"projects/954273464710/locations/global/collections/default_collection/dataStores/category_1732854489356/branches/0/documents/56358b1f5f740645e4ecb88e80188ee1\"\n",
            "    id: \"56358b1f5f740645e4ecb88e80188ee1\"\n",
            "    derived_struct_data {\n",
            "      fields {\n",
            "        key: \"snippets\"\n",
            "        value {\n",
            "          list_value {\n",
            "            values {\n",
            "              struct_value {\n",
            "                fields {\n",
            "                  key: \"snippet\"\n",
            "                  value {\n",
            "                    string_value: \"No snippet is available for this page.\"\n",
            "                  }\n",
            "                }\n",
            "                fields {\n",
            "                  key: \"snippet_status\"\n",
            "                  value {\n",
            "                    string_value: \"NO_SNIPPET_AVAILABLE\"\n",
            "                  }\n",
            "                }\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "total_size: 1193\n",
            "attribution_token: \"9gHw9QoMCMX6z7oGEO-D58sCEiQ2NzVlY2YwZC0wMDAwLTIwNGItYjY1Ny0xNDIyM2JjNWI2M2UiB0dFTkVSSUMqtAGjibMtkKS0MJnd2DC0kq4wg7KaIubdxDCuxIot6IKxLbe3jC3HxrEwqvizLdHmtS_p3cQwoImzLZneqC_Usp0VrfizLcH4vDCY1rctq8SKLeuCsS2AspoixMaxMJSSxTDE_MswxcvzF6OAlyLn7Ygtt5KuMPn2sy2NpLQwwvCeFduPmiKOkckwnN3YMMT4vDCb1rctjr6dFd6PmiKW3qgvkPeyMMH8yzDO5rUv_PazLeTtiC0wAQ\"\n",
            "next_page_token: \"U2M2IWNjJ2MyIDNx0yN1YjYtIGNwITLwADMw0yYwY2YlVzN2QiGDA4wvCPEGo73jXMCMIBMxIgC\"\n",
            "summary {\n",
            "  summary_text: \"It sounds like you were unable to work on something today. [1, 3] It\\'s okay to take breaks and prioritize your well-being. [2, 4] You can resume your work when you\\'re able. [2, 4]  It\\'s important to find a balance and work at your own pace. [5]  Keep up the good work! [3] \\n\"\n",
            "  summary_with_metadata {\n",
            "    summary: \"It sounds like you were unable to work on something today. It\\'s okay to take breaks and prioritize your well-being. You can resume your work when you\\'re able.  It\\'s important to find a balance and work at your own pace.  Keep up the good work! \\n\"\n",
            "    citation_metadata {\n",
            "      citations {\n",
            "        end_index: 58\n",
            "        sources {\n",
            "        }\n",
            "        sources {\n",
            "          reference_index: 2\n",
            "        }\n",
            "      }\n",
            "      citations {\n",
            "        start_index: 59\n",
            "        end_index: 115\n",
            "        sources {\n",
            "          reference_index: 1\n",
            "        }\n",
            "        sources {\n",
            "          reference_index: 3\n",
            "        }\n",
            "      }\n",
            "      citations {\n",
            "        start_index: 116\n",
            "        end_index: 158\n",
            "        sources {\n",
            "          reference_index: 1\n",
            "        }\n",
            "        sources {\n",
            "          reference_index: 3\n",
            "        }\n",
            "      }\n",
            "      citations {\n",
            "        start_index: 160\n",
            "        end_index: 219\n",
            "        sources {\n",
            "          reference_index: 4\n",
            "        }\n",
            "      }\n",
            "      citations {\n",
            "        start_index: 221\n",
            "        end_index: 243\n",
            "        sources {\n",
            "          reference_index: 2\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    references {\n",
            "      document: \"projects/954273464710/locations/global/collections/default_collection/dataStores/category_1732854489356/branches/0/documents/e36874abcb88c00019bbf57dc8a4ae00\"\n",
            "    }\n",
            "    references {\n",
            "      document: \"projects/954273464710/locations/global/collections/default_collection/dataStores/category_1732854489356/branches/0/documents/7e0390da41090783733bfa101510b0f0\"\n",
            "    }\n",
            "    references {\n",
            "      document: \"projects/954273464710/locations/global/collections/default_collection/dataStores/category_1732854489356/branches/0/documents/c72af80c21313c66b3acaf73b59d9b2f\"\n",
            "    }\n",
            "    references {\n",
            "      document: \"projects/954273464710/locations/global/collections/default_collection/dataStores/category_1732854489356/branches/0/documents/38901678e0c12d8edfb1f8de054973d5\"\n",
            "    }\n",
            "    references {\n",
            "      document: \"projects/954273464710/locations/global/collections/default_collection/dataStores/category_1732854489356/branches/0/documents/57369d0204541707a7e06f7efe0cf5ce\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "query_expansion_info {\n",
            "}\n",
            ">\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "google.cloud.discoveryengine_v1.services.search_service.pagers.SearchPager"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(search_sample(\n",
        "project_id = \"monobrain-development\",\n",
        "location = \"global\",                    # Values: \"global\", \"us\", \"eu\"\n",
        "engine_id = \"sample-category_1732854393042\",\n",
        "search_query = \"今日は取り組めませんでした\"\n",
        "))\n",
        "type(search_sample(\n",
        "project_id = \"monobrain-development\",\n",
        "location = \"global\",                    # Values: \"global\", \"us\", \"eu\"\n",
        "engine_id = \"sample-category_1732854393042\",\n",
        "search_query = \"今日は取り組めませんでした\"\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "qqWCSlzES8gR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 82567,
          "status": "ok",
          "timestamp": 1733558364211,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "qqWCSlzES8gR",
        "outputId": "3eeed119-a914-4697-d87a-f5bbe0cef9f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average score of this model is: 2.88\n",
            "\n",
            "The average time taken for RAG retrieval is: 1.0700 seconds\n"
          ]
        }
      ],
      "source": [
        "from google.cloud.discoveryengine_v1.services.search_service.pagers import SearchPager\n",
        "\n",
        "from google.cloud.discoveryengine_v1.services.search_service.pagers import SearchPager\n",
        "\n",
        "\n",
        "# Transform the response into desired format\n",
        "def transform_search_response(response) -> list:\n",
        "    similar_chats = []\n",
        "    for result in response.results:\n",
        "        data = dict(result.document.struct_data)\n",
        "        chat = {\n",
        "            'id': data.get('id', ''),\n",
        "            'question': data.get('question', ''),\n",
        "            'answer': data.get('answer', ''),\n",
        "            'category': data.get('category', ''),\n",
        "        }\n",
        "        similar_chats.append(chat)\n",
        "    return similar_chats\n",
        "\n",
        "# Fetch data from multiple pages\n",
        "def fetch_all_data(project_id, location, engine_id, num_pages=5, page_size=20):\n",
        "    client_options = (\n",
        "        ClientOptions(api_endpoint=f\"{location}-discoveryengine.googleapis.com\")\n",
        "        if location != \"global\"\n",
        "        else None\n",
        "    )\n",
        "\n",
        "    client = discoveryengine.SearchServiceClient(client_options=client_options)\n",
        "    serving_config = f\"projects/{project_id}/locations/{location}/collections/default_collection/engines/{engine_id}/servingConfigs/default_config\"\n",
        "\n",
        "    all_data = []\n",
        "    for page in range(num_pages):\n",
        "        request = discoveryengine.SearchRequest(\n",
        "            serving_config=serving_config,\n",
        "            query=\"\",  # Broad query to fetch data\n",
        "            page_size=page_size,\n",
        "            offset=page * page_size\n",
        "        )\n",
        "        response = client.search(request)\n",
        "        all_data.extend(transform_search_response(response))\n",
        "\n",
        "    return all_data\n",
        "\n",
        "# Function to assess similarity between two queries using Vertex AI\n",
        "def similarity_assessment_model(query1, query2):\n",
        "    generation_config = {\n",
        "        \"max_output_tokens\": 8192,\n",
        "        \"temperature\": 1,\n",
        "        \"top_p\": 0.95,\n",
        "        \"response_mime_type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    vertexai.init(project=\"monobrain-development\", location=\"us-central1\")\n",
        "    model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
        "    responses = model.generate_content([\n",
        "        \"\"\"あなたは日本語の小説の編集者をしており、文章のスペシャリストです。\n",
        "        以下の文章を比べて類似度を数値5段階で出力してください。\n",
        "\n",
        "        文章の大まかな内容と細かい内容、文章表現全てが類似している場合は : 5\n",
        "        文章の大まかな内容は類似しているが、細かい内容が違っており、文章表現が類似しているものは : 4\n",
        "        文章の大まかな内容は類似しているが、細かい内容が違っており、文章表現が異なっているものは : 3\n",
        "        文章の大まかな内容は異なっているが、文内で使われている単語が似通っている場合 : 2\n",
        "        文書の内容は間違っているが、文章表現や文章の雰囲気が似ている場合 : 1\n",
        "        文章の内容が違っており、文章表現や単語も異なっている場合 : 0\n",
        "\n",
        "        jsonの形式は{\"similarity\":3,\"comment\":\"ここになぜそのような評価にしたかのコメント\"}\n",
        "\n",
        "        文章1 :\"\"\" + query1 + \"\\n 文章2 : \" + query2],\n",
        "        generation_config=generation_config,\n",
        "        stream=False,\n",
        "    )\n",
        "\n",
        "    score = float(json.loads(responses.candidates[0].content.parts[0].text)['similarity'])\n",
        "    return score\n",
        "\n",
        "# Step 1: Fetch data from multiple pages\n",
        "all_data = fetch_all_data(\n",
        "    project_id=\"monobrain-development\",\n",
        "    location=\"global\",\n",
        "    engine_id=\"sample-category_1732854393042\",\n",
        "    num_pages=5,\n",
        "    page_size=20\n",
        ")\n",
        "\n",
        "# Step 2: Randomly select 20 entries for evaluation\n",
        "evaluation_data = random.sample(all_data, min(20, len(all_data)))\n",
        "\n",
        "# Step 3: Evaluate each query and find 5 similar questions\n",
        "similarity_scores = []\n",
        "total_time = 0.0\n",
        "\n",
        "for idx, data in enumerate(evaluation_data, start=1):\n",
        "    query = data['question']\n",
        "\n",
        "    # Query the engine for similar questions\n",
        "    start_time = time.time()\n",
        "    similar_response = search_sample(\n",
        "        project_id=\"monobrain-development\",\n",
        "        location=\"global\",\n",
        "        engine_id=\"sample-category_1732854393042\",\n",
        "        search_query=query\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    time_for_using_rag = end_time - start_time\n",
        "    total_time += time_for_using_rag\n",
        "\n",
        "    # Transform the response to extract similar chats\n",
        "    similar_chats = transform_search_response(similar_response)[:5]\n",
        "\n",
        "    for i, chat in enumerate(similar_chats, start=1):\n",
        "        similarity_score = similarity_assessment_model(query, chat['question'])\n",
        "        similarity_scores.append(similarity_score)\n",
        "\n",
        "# Calculate the average similarity score\n",
        "if similarity_scores:\n",
        "    average_similarity = np.mean(similarity_scores)\n",
        "else:\n",
        "    average_similarity = 0.0\n",
        "\n",
        "# Calculate the average time taken for RAG retrieval\n",
        "average_time = total_time / len(evaluation_data)\n",
        "\n",
        "# Output the results\n",
        "print(f\"\\nThe average score of this model is: {average_similarity:.2f}\")\n",
        "print(f\"\\nThe average time taken for RAG retrieval is: {average_time:.4f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qHYBJrLOIC8c",
      "metadata": {
        "id": "qHYBJrLOIC8c"
      },
      "source": [
        "Without Category Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "IHQUAEYJIFLK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1849,
          "status": "ok",
          "timestamp": 1733558600394,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "IHQUAEYJIFLK",
        "outputId": "dd7984ad-f50c-40fe-b19c-98c8521b3ea3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SearchPager<results {\n",
            "  id: \"2f2fda00a42f572696cc306f99c94c53\"\n",
            "  document {\n",
            "    struct_data {\n",
            "      fields {\n",
            "        key: \"question\"\n",
            "        value {\n",
            "          string_value: \"谷津さんお久しぶりです1週間程体調を崩してしまい、あまり進められませんでした。取り組みもご報告できておらず、すみませんでした。本日からまた頑張っていきます！取り組んだこと10/4・2-2 動画視聴10/5・2-2 実践課題実施10/6・2-2 実践課題提出・2-3 動画視聴10/8・勉強会・2-3 実践課題実施、提出10/14・2-3 実践課題実施、再提出・3-1 動画視聴、実践課題提出・3-2 動画視聴、実践課題提出・3-3 動画視聴、実践課題実施\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"id\"\n",
            "        value {\n",
            "          string_value: \"qa_1341\"\n",
            "        }\n",
            "      }\n",
            "      fields {\n",
            "        key: \"answer\"\n",
            "        value {\n",
            "          string_value: \"ご報告ありがとうございます！体調不良の中でも、コツコツ進めてくださり素晴らしいです🥲体調第一で、引き続きご自身の良いペースで無理はしすぎず進めていきましょう😊お取り組みお疲れ様でした👏✨\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    name: \"projects/954273464710/locations/global/collections/default_collection/dataStores/query_1732851097294/branches/0/documents/2f2fda00a42f572696cc306f99c94c53\"\n",
            "    id: \"2f2fda00a42f572696cc306f99c94c53\"\n",
            "    derived_struct_data {\n",
            "      fields {\n",
            "        key: \"snippets\"\n",
            "        value {\n",
            "          list_value {\n",
            "            values {\n",
            "              struct_value {\n",
            "                fields {\n",
            "                  key: \"snippet\"\n",
            "                  value {\n",
            "                    string_value: \"No snippet is available for this page.\"\n",
            "                  }\n",
            "                }\n",
            "                fields {\n",
            "                  key: \"snippet_status\"\n",
            "                  value {\n",
            "                    string_value: \"NO_SNIPPET_AVAILABLE\"\n",
            "                  }\n",
            "                }\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "total_size: 1\n",
            "attribution_token: \"9gHw9QoMCMaC0LoGEL2bgc8BEiQ2NzUyM2E1Ny0wMDAwLTJjMjktODgxZC0xNDIyM2JjOTZjYTIiB0dFTkVSSUMqtAH59rMtg7KaIreSrjCNpLQw24-aIsT4vDCuxIotmd3YMMTGsTCjibMtzua1L46RyTCrxIotjr6dFcfL8xf89rMtwfi8MMH8yzCgibMtm9a3LYCymiKW3qgv3o-aIpzd2DDR5rUv5-2ILar4sy3HxrEwwvCeFZCktDDogrEtt7eMLaOAlyK0kq4w5O2ILa34sy2Z3qgv1LKdFebdxDCY1rctlJLFMOndxDDrgrEtxPzLMJD3sjAwAQ\"\n",
            "summary {\n",
            "  summary_text: \"I understand you were unable to work on the task today. [1] It\\'s great that you\\'re still making progress despite feeling unwell. Please prioritize your health and continue at your own pace. [1] You\\'ve been working hard, so take a break if you need it.  We appreciate your efforts and look forward to seeing your continued progress. [1] \\n\"\n",
            "  summary_with_metadata {\n",
            "    summary: \"I understand you were unable to work on the task today. It\\'s great that you\\'re still making progress despite feeling unwell. Please prioritize your health and continue at your own pace. You\\'ve been working hard, so take a break if you need it.  We appreciate your efforts and look forward to seeing your continued progress. \\n\"\n",
            "    citation_metadata {\n",
            "      citations {\n",
            "        end_index: 55\n",
            "        sources {\n",
            "        }\n",
            "      }\n",
            "      citations {\n",
            "        start_index: 56\n",
            "        end_index: 185\n",
            "        sources {\n",
            "        }\n",
            "      }\n",
            "      citations {\n",
            "        start_index: 245\n",
            "        end_index: 323\n",
            "        sources {\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    references {\n",
            "      document: \"projects/954273464710/locations/global/collections/default_collection/dataStores/query_1732851097294/branches/0/documents/2f2fda00a42f572696cc306f99c94c53\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "query_expansion_info {\n",
            "}\n",
            ">\n"
          ]
        }
      ],
      "source": [
        "print(search_sample(\n",
        "project_id = \"monobrain-development\",\n",
        "location = \"global\",                    # Values: \"global\", \"us\", \"eu\"\n",
        "engine_id = \"samplequery_1732851004864\",\n",
        "search_query = \"今日は取り組めませんでした\"\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "Spd65XXAIR4v",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 59455,
          "status": "ok",
          "timestamp": 1733558687779,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -420
        },
        "id": "Spd65XXAIR4v",
        "outputId": "605a522e-f9f4-47a9-dd94-a322ecc4ab53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average score of this model is: 2.05\n",
            "\n",
            "The average time taken for RAG retrieval is: 0.9076 seconds\n"
          ]
        }
      ],
      "source": [
        "from google.cloud.discoveryengine_v1.services.search_service.pagers import SearchPager\n",
        "\n",
        "from google.cloud.discoveryengine_v1.services.search_service.pagers import SearchPager\n",
        "\n",
        "\n",
        "# Transform the response into desired format\n",
        "def transform_search_response(response) -> list:\n",
        "    similar_chats = []\n",
        "    for result in response.results:\n",
        "        data = dict(result.document.struct_data)\n",
        "        chat = {\n",
        "            'id': data.get('id', ''),\n",
        "            'question': data.get('question', ''),\n",
        "            'answer': data.get('answer', ''),\n",
        "            #'category': data.get('category', ''),\n",
        "        }\n",
        "        similar_chats.append(chat)\n",
        "    return similar_chats\n",
        "\n",
        "# Fetch data from multiple pages\n",
        "def fetch_all_data(project_id, location, engine_id, num_pages=5, page_size=20):\n",
        "    client_options = (\n",
        "        ClientOptions(api_endpoint=f\"{location}-discoveryengine.googleapis.com\")\n",
        "        if location != \"global\"\n",
        "        else None\n",
        "    )\n",
        "\n",
        "    client = discoveryengine.SearchServiceClient(client_options=client_options)\n",
        "    serving_config = f\"projects/{project_id}/locations/{location}/collections/default_collection/engines/{engine_id}/servingConfigs/default_config\"\n",
        "\n",
        "    all_data = []\n",
        "    for page in range(num_pages):\n",
        "        request = discoveryengine.SearchRequest(\n",
        "            serving_config=serving_config,\n",
        "            query=\"\",  # Broad query to fetch data\n",
        "            page_size=page_size,\n",
        "            offset=page * page_size\n",
        "        )\n",
        "        response = client.search(request)\n",
        "        all_data.extend(transform_search_response(response))\n",
        "\n",
        "    return all_data\n",
        "\n",
        "# Function to assess similarity between two queries using Vertex AI\n",
        "def similarity_assessment_model(query1, query2):\n",
        "    generation_config = {\n",
        "        \"max_output_tokens\": 8192,\n",
        "        \"temperature\": 1,\n",
        "        \"top_p\": 0.95,\n",
        "        \"response_mime_type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    vertexai.init(project=\"monobrain-development\", location=\"us-central1\")\n",
        "    model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
        "    responses = model.generate_content([\n",
        "        \"\"\"あなたは日本語の小説の編集者をしており、文章のスペシャリストです。\n",
        "        以下の文章を比べて類似度を数値5段階で出力してください。\n",
        "\n",
        "        文章の大まかな内容と細かい内容、文章表現全てが類似している場合は : 5\n",
        "        文章の大まかな内容は類似しているが、細かい内容が違っており、文章表現が類似しているものは : 4\n",
        "        文章の大まかな内容は類似しているが、細かい内容が違っており、文章表現が異なっているものは : 3\n",
        "        文章の大まかな内容は異なっているが、文内で使われている単語が似通っている場合 : 2\n",
        "        文書の内容は間違っているが、文章表現や文章の雰囲気が似ている場合 : 1\n",
        "        文章の内容が違っており、文章表現や単語も異なっている場合 : 0\n",
        "\n",
        "        jsonの形式は{\"similarity\":3,\"comment\":\"ここになぜそのような評価にしたかのコメント\"}\n",
        "\n",
        "        文章1 :\"\"\" + query1 + \"\\n 文章2 : \" + query2],\n",
        "        generation_config=generation_config,\n",
        "        stream=False,\n",
        "    )\n",
        "\n",
        "    score = float(json.loads(responses.candidates[0].content.parts[0].text)['similarity'])\n",
        "    return score\n",
        "\n",
        "# Step 1: Fetch data from multiple pages\n",
        "all_data = fetch_all_data(\n",
        "    project_id=\"monobrain-development\",\n",
        "    location=\"global\",\n",
        "    engine_id=\"samplequery_1732851004864\",\n",
        "    num_pages=5,\n",
        "    page_size=20\n",
        ")\n",
        "\n",
        "# Step 2: Randomly select 20 entries for evaluation\n",
        "evaluation_data = random.sample(all_data, min(20, len(all_data)))\n",
        "\n",
        "# Step 3: Evaluate each query and find 5 similar questions\n",
        "similarity_scores = []\n",
        "total_time = 0.0\n",
        "\n",
        "for idx, data in enumerate(evaluation_data, start=1):\n",
        "    query = data['question']\n",
        "\n",
        "    # Query the engine for similar questions\n",
        "    start_time = time.time()\n",
        "    similar_response = search_sample(\n",
        "        project_id=\"monobrain-development\",\n",
        "        location=\"global\",\n",
        "        engine_id=\"samplequery_1732851004864\",\n",
        "        search_query=query\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    time_for_using_rag = end_time - start_time\n",
        "    total_time += time_for_using_rag\n",
        "\n",
        "    # Transform the response to extract similar chats\n",
        "    similar_chats = transform_search_response(similar_response)[:5]\n",
        "\n",
        "    for i, chat in enumerate(similar_chats, start=1):\n",
        "        similarity_score = similarity_assessment_model(query, chat['question'])\n",
        "        similarity_scores.append(similarity_score)\n",
        "\n",
        "# Calculate the average similarity score\n",
        "if similarity_scores:\n",
        "    average_similarity = np.mean(similarity_scores)\n",
        "else:\n",
        "    average_similarity = 0.0\n",
        "\n",
        "# Calculate the average time taken for RAG retrieval\n",
        "average_time = total_time / len(evaluation_data)\n",
        "\n",
        "# Output the results\n",
        "print(f\"\\nThe average score of this model is: {average_similarity:.2f}\")\n",
        "print(f\"\\nThe average time taken for RAG retrieval is: {average_time:.4f} seconds\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "rag_score_evaluation_and_speed_analysys20_2024_12_02",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
    
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
